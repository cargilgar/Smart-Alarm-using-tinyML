{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_Training.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNr8XNp9+AWs7/bh6mHsifS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cargilgar/Smart-Alarm-using-tinyML/blob/main/Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAX4NMxXBauV"
      },
      "source": [
        "1.- Convert and reformat accelerometer data into a time-sliced representation\n",
        "\n",
        "2.- Visualize the accelerometer data\n",
        "\n",
        "3.- Reshape the multi-dimensional tabular data so that it is accepted by Keras\n",
        "\n",
        "4.- Split up the data set into training, validation, and test set\n",
        "\n",
        "5.- Define a deep neural network model in Keras which can later be processed by Apple’s Core ML\n",
        "\n",
        "6.- Train the deep neural network for human activity recognition data\n",
        "\n",
        "7.- Validate the performance of the trained DNN against the test data using learning curve and confusion matrix\n",
        "\n",
        "8.- Export the trained Keras DNN model for Core ML\n",
        "\n",
        "9.- Ensure that the Core ML model was exported correctly by conducting a sample prediction in Python\n",
        "\n",
        "\n",
        "10.- Use Apple’s Core ML library in order to predict the outcomes for a given data set using Swift"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMspG4Ee4ykf"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d26tssDbHuhi"
      },
      "source": [
        "# **Split Dataset**\n",
        "\n",
        "Train, Validation and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKbziufL71Q-"
      },
      "source": [
        "def readucr(filename):\n",
        "    data = np.loadtxt(filename, delimiter=\"\\t\")\n",
        "    y = data[:, 0]\n",
        "    x = data[:, 1:]\n",
        "    return x, y.astype(int)\n",
        "\n",
        "\n",
        "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
        "\n",
        "x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
        "x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dumte5WoIgvC"
      },
      "source": [
        "# Build the model\n",
        "We build a Convolutional Neural Network.\n",
        "The implementation is based on the TF 2 version.*texto en cursiva*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhJpl6hlIf42"
      },
      "source": [
        "def make_model(input_shape):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
        "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "    conv1 = keras.layers.ReLU()(conv1)\n",
        "\n",
        "    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
        "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "    conv2 = keras.layers.ReLU()(conv2)\n",
        "\n",
        "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
        "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
        "    conv3 = keras.layers.ReLU()(conv3)\n",
        "\n",
        "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
        "\n",
        "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "model = make_model(input_shape=x_train.shape[1:])\n",
        "keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM_JiOZBJLV7"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2HhwnDHJKBN"
      },
      "source": [
        "epochs = 500\n",
        "batch_size = 32\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
        "]\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_split=0.2,\n",
        "    verbose=1,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRqTDxxCJPIH"
      },
      "source": [
        "# Evaluate model on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37V5g3CgJOoY"
      },
      "source": [
        "model = keras.models.load_model(\"best_model.h5\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"Test accuracy\", test_acc)\n",
        "print(\"Test loss\", test_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uBd2AhxJUMN"
      },
      "source": [
        "# Plot the model's training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k731PX_QJTr9"
      },
      "source": [
        "metric = \"sparse_categorical_accuracy\"\n",
        "plt.figure()\n",
        "plt.plot(history.history[metric])\n",
        "plt.plot(history.history[\"val_\" + metric])\n",
        "plt.title(\"model \" + metric)\n",
        "plt.ylabel(metric, fontsize=\"large\")\n",
        "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
        "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}