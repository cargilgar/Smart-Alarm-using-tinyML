{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOCF34sKVEHWdZaRJFpztFp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cargilgar/Smart-Alarm-using-tinyML/blob/main/Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iB8fV6iWeQs"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uge8UaRLVvuT",
        "outputId": "2cb11cde-0327-4420-957f-cb4bbc1407f4"
      },
      "source": [
        "data_path = os.path.join(os.getcwd(), \"Dataset/\")\n",
        "data_list = sorted(os.listdir(data_path))\n",
        "data_list[0]\n",
        "\n",
        "read_dataset = pd.read_csv(os.path.join(data_path, data_list[0]), delimiter=',')\n",
        "#read_dataset\n",
        "\n",
        "total_rows=len(read_dataset.axes[0]) #===> Axes of 0 is for a row\n",
        "total_cols=len(read_dataset.axes[1]) #===> Axes of 1 is for a column\n",
        "print(\"Number of Rows: \"+str(total_rows))\n",
        "print(\"Number of Columns: \"+str(total_cols))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Rows: 50\n",
            "Number of Columns: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy7rqDSxWfNW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAX4NMxXBauV"
      },
      "source": [
        "1.- Split up the data set into training, validation, and test set\n",
        "\n",
        "5.- Define a deep neural network model in Keras which can later be processed by Apple’s Core ML\n",
        "\n",
        "6.- Train the deep neural network for human activity recognition data\n",
        "\n",
        "7.- Validate the performance of the trained DNN against the test data using learning curve and confusion matrix\n",
        "\n",
        "8.- Export the trained Keras DNN model for Core ML\n",
        "\n",
        "9.- Ensure that the Core ML model was exported correctly by conducting a sample prediction in Python\n",
        "\n",
        "\n",
        "10.- Use Apple’s Core ML library in order to predict the outcomes for a given data set using Swift"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMspG4Ee4ykf"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC30xxnNJ70d"
      },
      "source": [
        "# Standarize the data\n",
        "Our timeseries are already in a single length (176). However, their values are usually in various ranges. This is not ideal for a neural network; in general we should seek to make the input values normalized. For this specific dataset, the data is already z-normalized: each timeseries sample has a mean equal to zero and a standard deviation equal to one. This type of normalization is very common for timeseries classification problems, see Bagnall et al. (2016).\n",
        "\n",
        "Note that the timeseries data used here are univariate, meaning we only have one channel per timeseries example. We will therefore transform the timeseries into a multivariate one with one channel using a simple reshaping via numpy. This will allow us to construct a model that is easily applicable to multivariate time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkmtHfSPKDGt"
      },
      "source": [
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8rIsZeKKNnA"
      },
      "source": [
        "Finally, in order to use sparse_categorical_crossentropy, we will have to count the number of classes beforehand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vi2lpW1KExQ"
      },
      "source": [
        "num_classes = len(np.unique(y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G7kPrQWKL88"
      },
      "source": [
        "Now we shuffle the training set because we will be using the validation_split option later when training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APAAU9zoKGCt"
      },
      "source": [
        "idx = np.random.permutation(len(x_train))\n",
        "x_train = x_train[idx]\n",
        "y_train = y_train[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHGEVuCTKKVq"
      },
      "source": [
        "Standardize the labels to positive integers. The expected labels will then be 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJXDNZFNKHoY"
      },
      "source": [
        "y_train[y_train == -1] = 0\n",
        "y_test[y_test == -1] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d26tssDbHuhi"
      },
      "source": [
        "# **Split Dataset**\n",
        "\n",
        "Train, Validation and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn7D3RdHL1CT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKbziufL71Q-"
      },
      "source": [
        "def format_image(image, label):\n",
        "    image = tf.image.resize(image, (224, 224)) / 255.0\n",
        "    return  image, label\n",
        "\n",
        "\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
        "    'rock_paper_scissors', split=['train[:80%]', 'train[80%:]', 'test'], \n",
        "    with_info=True, as_supervised=True)\n",
        "num_examples = metadata.splits['train'].num_examples\n",
        "num_classes = metadata.features['label'].num_classes\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_batches = raw_train.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "test_batches = raw_test.map(format_image).batch(1)\n",
        "\n",
        "for image_batch, label_batch in train_batches.take(1):\n",
        "    pass"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ9krAqfKT6Y"
      },
      "source": [
        "# Visualize the data\n",
        "Here we visualize one timeseries example for each class in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF44xXwsKSfo"
      },
      "source": [
        "classes = np.unique(np.concatenate((y_train, y_test), axis=0))\n",
        "\n",
        "plt.figure()\n",
        "for c in classes:\n",
        "    c_x_train = x_train[y_train == c]\n",
        "    plt.plot(c_x_train[0], label=\"class \" + str(c))\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dumte5WoIgvC"
      },
      "source": [
        "# Build the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhJpl6hlIf42"
      },
      "source": [
        "module_selection = (\"mobilenet_v2\", 224, 1280) \n",
        "handle_base, pixels, FV_SIZE = module_selection\n",
        "MODULE_HANDLE =\"https://tfhub.dev/google/tf2-preview/{}/feature_vector/4\".format(handle_base)\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(\"Using {} with input size {} and output dimension {}\".format(MODULE_HANDLE, IMAGE_SIZE, FV_SIZE))\n",
        "\n",
        "feature_extractor = hub.KerasLayer(MODULE_HANDLE,\n",
        "                                   input_shape=IMAGE_SIZE + (3,), \n",
        "                                   output_shape=[FV_SIZE],\n",
        "                                   trainable=False)\n",
        "\n",
        "print(\"Building model with\", MODULE_HANDLE)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "        feature_extractor,\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "EPOCHS = 15\n",
        "\n",
        "hist = model.fit(train_batches,\n",
        "                 epochs=EPOCHS,\n",
        "                 validation_data=validation_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM_JiOZBJLV7"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2HhwnDHJKBN"
      },
      "source": [
        "epochs = 500\n",
        "batch_size = 32\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
        "]\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_split=0.2,\n",
        "    verbose=1,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRqTDxxCJPIH"
      },
      "source": [
        "# Evaluate model on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37V5g3CgJOoY"
      },
      "source": [
        "model = keras.models.load_model(\"best_model.h5\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"Test accuracy\", test_acc)\n",
        "print(\"Test loss\", test_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uBd2AhxJUMN"
      },
      "source": [
        "# Plot the model's training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k731PX_QJTr9"
      },
      "source": [
        "metric = \"sparse_categorical_accuracy\"\n",
        "plt.figure()\n",
        "plt.plot(history.history[metric])\n",
        "plt.plot(history.history[\"val_\" + metric])\n",
        "plt.title(\"model \" + metric)\n",
        "plt.ylabel(metric, fontsize=\"large\")\n",
        "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
        "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}