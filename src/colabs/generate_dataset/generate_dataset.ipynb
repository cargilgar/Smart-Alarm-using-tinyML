{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1pH40itHTdra",
   "metadata": {
    "id": "1pH40itHTdra"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Carlos Gil, Daniel Moreno.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ac485",
   "metadata": {
    "id": "a52ac485"
   },
   "source": [
    "## Dataset download\n",
    "Downloading the dataset from `https://physionet.org` and storing it locally in a folder called `download/`\n",
    "\n",
    "**Note:** If the dataset has been downloaded, you don't need to run the following cell.\n",
    "\n",
    "You can download the dataset manually by going to the [website](https://www.physionet.org/content/sleep-accel/1.0.0/ \"Physionet\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PS7-hk5PTbUU",
   "metadata": {
    "id": "PS7-hk5PTbUU",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install wget\n",
    "\n",
    "!wget -r -nv -N -c -np https://physionet.org/files/sleep-accel/1.0.0/\n",
    "\n",
    "!mkdir ./download\n",
    "\n",
    "!mv ./physionet.org/files/sleep-accel/1.0.0/* download\n",
    "\n",
    "!rm -r ./physionet.org/\n",
    "\n",
    "!find ./download -name \"*.html\" -type f -delete\n",
    "\n",
    "!mkdir ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "forbidden-forty",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1627643839942,
     "user": {
      "displayName": "Carlos Gil",
      "photoUrl": "",
      "userId": "12309568779801871703"
     },
     "user_tz": -120
    },
    "id": "forbidden-forty"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "qmXPkoZWyyEm",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1627643839943,
     "user": {
      "displayName": "Carlos Gil",
      "photoUrl": "",
      "userId": "12309568779801871703"
     },
     "user_tz": -120
    },
    "id": "qmXPkoZWyyEm"
   },
   "outputs": [],
   "source": [
    "class Error(Enum):\n",
    "    dir_not_empty = \"[Error]: the directory is not empty \"\n",
    "    match_number_users = \"[Error]: number of users in list does not match \"\n",
    "    match_user_id = \"[Error]: user id does not match between lists \"\n",
    "    match_length_arrays = \"[Error]: the length of the lists does not match \"\n",
    "    match_index = \"[Error]: the indexes are mismatched \"\n",
    "    duplicated = \"[Error]: found duplicated indexes \"\n",
    "    wrong_value = \"[Error]: wrong value introduced \"\n",
    "    generic_error = \"[Error] \"\n",
    "\n",
    "    @staticmethod\n",
    "    def raise_error(type_error, value):\n",
    "        return type_error.value + \": \" + str(value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cGfE03A-Uvvb",
   "metadata": {
    "id": "cGfE03A-Uvvb"
   },
   "source": [
    "## Layout and paths preparation\n",
    "\n",
    "The directory tree should resemble the following layout structure:\n",
    "\n",
    "```bash\n",
    "dataset/\n",
    "├── download/\n",
    "│   ├── heart_rate/\n",
    "│   ├── labels/\n",
    "│   └── steps/\n",
    "└── output/\n",
    "    ├── dataset_user_1.csv\n",
    "    ├── dataset_user_2.csv\n",
    "    ⁞\n",
    "    └── dataset_user_31.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "expanded-flood",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1627643839943,
     "user": {
      "displayName": "Carlos Gil",
      "photoUrl": "",
      "userId": "12309568779801871703"
     },
     "user_tz": -120
    },
    "id": "expanded-flood"
   },
   "outputs": [],
   "source": [
    "# Output path.\n",
    "output_dir = 'output'\n",
    "\n",
    "output_path = os.path.join(os.getcwd(), output_dir)\n",
    "\n",
    "# Download path.\n",
    "data_path = os.path.join(os.getcwd(), 'download')\n",
    "\n",
    "motion_path = os.path.join(data_path, 'motion')\n",
    "heart_rate_path = os.path.join(data_path, 'heart_rate')\n",
    "labels_path = os.path.join(data_path, 'labels')\n",
    "\n",
    "# Obtaining lists with all users in ascending ordered\n",
    "motion_list = sorted(os.listdir(motion_path))\n",
    "heart_rate_list = sorted(os.listdir(heart_rate_path))\n",
    "labels_list = sorted(os.listdir(labels_path))\n",
    "\n",
    "# Checking that the downloaded data contains the 31 users in all the lists created\n",
    "assert len(motion_list) == 31, Error.match_number_users.value\n",
    "assert len(heart_rate_list) == 31, Error.match_number_users.value\n",
    "assert len(labels_list) == 31, Error.match_number_users.value\n",
    "\n",
    "user_ids = []\n",
    "\n",
    "# Checking that the user ids match in order accross the three lists\n",
    "for item in range(len(motion_list)):\n",
    "    user_motion_id = re.search(\"\\d*\", motion_list[item])\n",
    "    user_heart_rate_id = re.search(\"\\d*\", heart_rate_list[item])\n",
    "    user_labels_id = re.search(\"\\d*\", labels_list[item])\n",
    "\n",
    "    assert user_motion_id.group(0) == user_heart_rate_id.group(0), Error.match_user_id.value\n",
    "    assert user_motion_id.group(0) == user_labels_id.group(0), Error.match_user_id.value\n",
    "\n",
    "    user_ids.append(user_motion_id.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31c16de",
   "metadata": {
    "id": "d31c16de"
   },
   "source": [
    "## Creation of generate datasets function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "GM9GSzduN12Z",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1627643839944,
     "user": {
      "displayName": "Carlos Gil",
      "photoUrl": "",
      "userId": "12309568779801871703"
     },
     "user_tz": -120
    },
    "id": "GM9GSzduN12Z"
   },
   "outputs": [],
   "source": [
    "def generate_all_datasets(labels_df=[], interval=0, verbose=False):\n",
    "    '''\n",
    "    This function takes a total of four parameters: \n",
    "        - labels_df: a list with the name of the columns of the generated datasets (6 labels). If not specified or less/more \n",
    "          than 6 labels given, it takes default values.\n",
    "        - verbose: if set to true, display some feedback of the process (the whole process might take several minutes).\n",
    "    '''\n",
    "    \n",
    "    # Checking that the output directory is empty\n",
    "    if len(os.listdir(output_dir)) > 0:\n",
    "        print(f\"The folder {output_dir}/ is not empty\")\n",
    "        input_val = input(\"Do you want to overwrite it? [y/n] \\n\")\n",
    "\n",
    "        if input_val == \"y\":\n",
    "            file_list = [file for file in os.listdir(output_dir)]\n",
    "            \n",
    "            for file in file_list:\n",
    "                os.remove(os.path.join(output_dir, file))\n",
    "        else:\n",
    "            raise Exception(Error.raise_error(Error.dir_not_empty, output_dir))\n",
    "\n",
    "    if len(labels_df) != 6:\n",
    "        labels_df = ['Time', 'X', 'Y', 'Z', 'Heart Rate', 'Labels']\n",
    "\n",
    "    time_acc = []\n",
    "\n",
    "    for user in range(0, len(user_ids)):\n",
    "        \n",
    "        start = time.time()\n",
    "\n",
    "        motion_user = os.path.join(motion_path, motion_list[user])\n",
    "        heart_rate_user = os.path.join(heart_rate_path, heart_rate_list[user])\n",
    "        labels_user = os.path.join(labels_path, labels_list[user])\n",
    "\n",
    "        user_df = generate_dataset(motion_user, heart_rate_user, labels_user, labels_df, interval)\n",
    "\n",
    "        if interval == 0 and user_df[\"Time\"].duplicated().any():\n",
    "            raise Exception(Error.raise_error(Error.duplicated, user_df))\n",
    "        \n",
    "        # Saving the generated dataset\n",
    "        fname = os.path.join(output_path, 'dataset_' + user_ids[user] + \".csv\")\n",
    "\n",
    "        user_df.to_csv(fname, index = False, header=True)\n",
    "\n",
    "        time_acc.append(time.time() - start)\n",
    "\n",
    "        if verbose:\n",
    "            print('Dataset user id <{}> generated succesfully. Time: {:.0f} s'.format(user_ids[user], time_acc[user]))\n",
    "\n",
    "\n",
    "    print(f'\\nProcess completed! Total time execution: {datetime.timedelta(seconds=sum(time_acc))}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "BEJ7CGoNwbFY",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1627643839945,
     "user": {
      "displayName": "Carlos Gil",
      "photoUrl": "",
      "userId": "12309568779801871703"
     },
     "user_tz": -120
    },
    "id": "BEJ7CGoNwbFY"
   },
   "outputs": [],
   "source": [
    "def generate_dataset(motion_user, heart_rate_user, labels_user, labels_columns, interval):\n",
    "\n",
    "    '''\n",
    "    this function accepts three filenames from one user to generate the dataset as well as the pre-defined intervals.\n",
    "    '''\n",
    "    \n",
    "    # --- Loading the txt files\n",
    "    motion = np.loadtxt(motion_user)\n",
    "    heart_rate = np.loadtxt(heart_rate_user, delimiter=',')\n",
    "    labels = np.loadtxt(labels_user)\n",
    "\n",
    "    # --- Cropping to match the labelled list\n",
    "    motion = crop_to_offset(motion, labels)    \n",
    "    heart_rate = crop_to_offset(heart_rate, labels)\n",
    "\n",
    "    # np.savetxt(\"cropped_motion.txt\", motion)\n",
    "    # np.savetxt(\"cropped_heart_rate.txt\", heart_rate)\n",
    "\n",
    "    # --- Pre.processing\n",
    "    motion = get_peak_values(motion, interval)\n",
    "\n",
    "    # --- Merging\n",
    "    merged_arrays = merge_arrays(motion, heart_rate, labels)\n",
    "\n",
    "    # --- Importing merged to a pandas dataframe\n",
    "    data_frame = pd.DataFrame(merged_arrays, columns=labels_columns)\n",
    "\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-zimbabwe",
   "metadata": {
    "id": "touched-zimbabwe"
   },
   "source": [
    "The raw data recorded with the Apple Watch (motion and heart rate) contains continuous and uninterrupted measurements of one or more days, including the last night.\n",
    "\n",
    "Since the data corresponding to the last night underwent a proper labelling from the polysomnography (PSG) results, it is necessary to crop the raw data only to that night (i.e. the list with labels). Anything else will not be part of the generated dataset and will be therefore disregarded.\n",
    "\n",
    "This is handled by the function `crop_to_offset()`. This function carries out two tasks:\n",
    "\n",
    "1. It finds the last night measured within the array passed.\n",
    "2. For the last night, it finds the boundaries corresponding to the start and end of the labelled list.\n",
    "\n",
    "Then, the function returns the indexes where the array needs to be sliced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "diagnostic-cooperative",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1627643839946,
     "user": {
      "displayName": "Carlos Gil",
      "photoUrl": "",
      "userId": "12309568779801871703"
     },
     "user_tz": -120
    },
    "id": "diagnostic-cooperative"
   },
   "outputs": [],
   "source": [
    "def crop_to_offset(array_to_crop, array_ref):\n",
    "    '''\n",
    "    This function takes two arrays, the first is the one to be cropped and the second one the reference to where to crop.\n",
    "    It returs a new array starting and ending where the indexes matched with the reference array.\n",
    "    '''\n",
    "\n",
    "    start_index, end_index = 0, 0\n",
    "    array_size = np.size(array_to_crop, 0)\n",
    "    cropped_array = []\n",
    "    \n",
    "    # --- Find the boundaries corresponding to the labelled list\n",
    "    first_item = array_ref[0][0]\n",
    "    last_item = array_ref[-1][0]\n",
    "    \n",
    "    last_item_found = False\n",
    "    \n",
    "    for item in range(array_size - 1, -1, -1):        \n",
    "        # find end index\n",
    "        if not last_item_found:\n",
    "            if array_to_crop[item][0] < last_item:\n",
    "                end_index = item + 1\n",
    "                last_item_found = True\n",
    "        \n",
    "        # find start index\n",
    "        if array_to_crop[item][0] < first_item:\n",
    "            start_index = item\n",
    "            break  # No more iteration is needed after finding end_index and start_index.\n",
    "    \n",
    "    cropped_array = array_to_crop[start_index:end_index]\n",
    "        \n",
    "    return cropped_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruO-YtshJmfx",
   "metadata": {
    "id": "ruO-YtshJmfx"
   },
   "source": [
    "In order to compress the vast amount of data gathered from the IMU sensor, some preprocessing  is required. Typically, these types of sensors record at a high frequencies resulting in hundreds of measurements every single second. Research suggests that changes among sleep cycles tend to occur gradually within a few seconds or even minutes. This also applies to the shift between NREM and REM, which is the variable of interest that is the focus of this application.\n",
    "\n",
    "Therefore, the time resolution in the raw dataset is too accurate, being most of the accelerometer data redundant or non-relevant and the function `get_peak_values()` will help us separate the wheat from the chaff. All this function does is to find the maximum values within a time interval (here 1 second). The number of maximum values are left to the user, being one maximum value per second as default. \n",
    "\n",
    "We consider these peak values are of much significance amongst all the accelerometer data for extracting the most meaningful features for training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "HLB9xAhnI3sb",
   "metadata": {
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1627643840244,
     "user": {
      "displayName": "Carlos Gil",
      "photoUrl": "",
      "userId": "12309568779801871703"
     },
     "user_tz": -120
    },
    "id": "HLB9xAhnI3sb"
   },
   "outputs": [],
   "source": [
    "def get_peak_values(array, interval=0):\n",
    "    '''\n",
    "    This function finds a specified number of peak values for every interval (1 second) of the passed array.\n",
    "    It returns the resulting processed array back.\n",
    "    '''\n",
    "\n",
    "    if interval < 0:\n",
    "        raise Exception(Error.raise_error(Error.wrong_value, \" negative interval time introduced\"))\n",
    "\n",
    "    array_size = np.size(array, 0)\n",
    "    max_value_x, max_value_y, max_value_z = 0, 0, 0\n",
    "    peak_values_x, peak_values_y, peak_values_z = [], [], []\n",
    "    \n",
    "    time_accumulate = []\n",
    "    \n",
    "    last_interval = 0\n",
    "\n",
    "    for item in range(array_size):\n",
    "        if (array[item][0] - last_interval) < interval:\n",
    "            if abs(array[item][1]) > abs(max_value_x):  # New peak value found at x\n",
    "                max_value_x = abs(array[item][1])\n",
    "\n",
    "            if abs(array[item][2]) > abs(max_value_y):  # New peak value found at y\n",
    "                max_value_y = abs(array[item][2])\n",
    "\n",
    "            if abs(array[item][3]) > abs(max_value_z):  # New peak value found at z\n",
    "                max_value_z = abs(array[item][3])        \n",
    "\n",
    "        else:\n",
    "            # end of interval\n",
    "            peak_values_x.append(max_value_x)\n",
    "            peak_values_y.append(max_value_y)\n",
    "            peak_values_z.append(max_value_z)\n",
    "\n",
    "            # reset interval values and increment count\n",
    "            last_interval = np.floor(array[item][0])\n",
    "\n",
    "            max_value_x = 0\n",
    "            max_value_y = 0\n",
    "            max_value_z = 0\n",
    "\n",
    "            last_interval = np.around(array[item][0], decimals=0)\n",
    "            time_accumulate.append(last_interval)\n",
    "        \n",
    "    \n",
    "    assert len(time_accumulate) == len(peak_values_x), Error.match_length_arrays.value\n",
    "    assert len(time_accumulate) == len(peak_values_y), Error.match_length_arrays.value\n",
    "    assert len(time_accumulate) == len(peak_values_z), Error.match_length_arrays.value\n",
    "    \n",
    "    return np.column_stack((time_accumulate, peak_values_x, peak_values_y, peak_values_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-buyer",
   "metadata": {
    "id": "growing-buyer"
   },
   "source": [
    "Last step is merging the already preprocessed motion dataset with the heart rate and labels datasets corresponding to each user. \n",
    "\n",
    "For the heart rate dataset the following approach is taken: For each window from the motion time interval, gather all the heart rate values that were measured within that interval and get the mean value of all of them. If no heart rate value was measured within a window interval, the last known value will be taken. If there is a time gap larger than the actual window's size, then discard that data up to the next available one.\n",
    "\n",
    "For the labels a simpler approach is taken: Since the time windows are meant to be always smaller or equal than the time interval of labels recorded, the label value will be unique. It can be either one that corresponds to that time window or the last one if missing in that interval. \n",
    "\n",
    "All the above-mentioned operations are carried out in the `merge_arrays()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "invalid-difference",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1627643840247,
     "user": {
      "displayName": "Carlos Gil",
      "photoUrl": "",
      "userId": "12309568779801871703"
     },
     "user_tz": -120
    },
    "id": "invalid-difference"
   },
   "outputs": [],
   "source": [
    "def merge_arrays(motion, heart_rate, labels):\n",
    "    \n",
    "    motion_size = np.size(motion, 0)\n",
    "    heart_rate_size = np.size(heart_rate, 0)\n",
    "    labels_size = np.size(labels, 0)\n",
    "\n",
    "    new_heart_rate = []\n",
    "    new_labels = []\n",
    "    \n",
    "    inc_heart_rate = 0\n",
    "    inc_labels = 0\n",
    "\n",
    "    for i in range(motion_size):\n",
    "\n",
    "        # Appending heart rate \n",
    "        if inc_heart_rate == heart_rate_size:\n",
    "            pass\n",
    "        elif motion[i][0] > heart_rate[inc_heart_rate][0]:\n",
    "            inc_heart_rate += 1\n",
    "        \n",
    "        new_heart_rate.append(heart_rate[inc_heart_rate - 1][1])\n",
    "\n",
    "        # Appending labels\n",
    "        if motion[i][0] > labels[inc_labels][0]:\n",
    "            inc_labels += 1\n",
    "\n",
    "        new_labels.append(labels[inc_labels - 1][1])\n",
    "\n",
    "        \n",
    "        # If the motion dataset finishes but there is still heart rate dataset, dismiss the rest of the latter.\n",
    "        if inc_heart_rate >= motion_size:\n",
    "            break\n",
    "\n",
    "        \n",
    "    assert motion_size == len(new_heart_rate), Error.match_length_arrays.value\n",
    "    assert motion_size == len(new_labels), Error.match_length_arrays.value\n",
    "    \n",
    "    return np.column_stack((motion, new_heart_rate, new_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54857484",
   "metadata": {
    "id": "54857484"
   },
   "source": [
    "## Running the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3c3633a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 858265,
     "status": "ok",
     "timestamp": 1627644698505,
     "user": {
      "displayName": "Carlos Gil",
      "photoUrl": "",
      "userId": "12309568779801871703"
     },
     "user_tz": -120
    },
    "id": "b3c3633a",
    "outputId": "9cfe3ee5-7d2d-47d2-cf2c-7c825a27faaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset user id <1066528> generated succesfully. Time: 24 s\n",
      "Dataset user id <1360686> generated succesfully. Time: 28 s\n",
      "Dataset user id <1449548> generated succesfully. Time: 32 s\n",
      "Dataset user id <1455390> generated succesfully. Time: 29 s\n",
      "Dataset user id <1818471> generated succesfully. Time: 29 s\n",
      "Dataset user id <2598705> generated succesfully. Time: 28 s\n",
      "Dataset user id <2638030> generated succesfully. Time: 35 s\n",
      "Dataset user id <3509524> generated succesfully. Time: 18 s\n",
      "Dataset user id <3997827> generated succesfully. Time: 32 s\n",
      "Dataset user id <4018081> generated succesfully. Time: 19 s\n",
      "Dataset user id <4314139> generated succesfully. Time: 28 s\n",
      "Dataset user id <4426783> generated succesfully. Time: 32 s\n",
      "Dataset user id <46343> generated succesfully. Time: 17 s\n",
      "Dataset user id <5132496> generated succesfully. Time: 20 s\n",
      "Dataset user id <5383425> generated succesfully. Time: 6 s\n",
      "Dataset user id <5498603> generated succesfully. Time: 23 s\n",
      "Dataset user id <5797046> generated succesfully. Time: 33 s\n",
      "Dataset user id <6220552> generated succesfully. Time: 33 s\n",
      "Dataset user id <759667> generated succesfully. Time: 19 s\n",
      "Dataset user id <7749105> generated succesfully. Time: 6 s\n",
      "Dataset user id <781756> generated succesfully. Time: 30 s\n",
      "Dataset user id <8000685> generated succesfully. Time: 29 s\n",
      "Dataset user id <8173033> generated succesfully. Time: 29 s\n",
      "Dataset user id <8258170> generated succesfully. Time: 6 s\n",
      "Dataset user id <844359> generated succesfully. Time: 27 s\n",
      "Dataset user id <8530312> generated succesfully. Time: 30 s\n",
      "Dataset user id <8686948> generated succesfully. Time: 28 s\n",
      "Dataset user id <8692923> generated succesfully. Time: 33 s\n",
      "Dataset user id <9106476> generated succesfully. Time: 29 s\n",
      "Dataset user id <9618981> generated succesfully. Time: 94 s\n",
      "Dataset user id <9961348> generated succesfully. Time: 28 s\n",
      "\n",
      "Process completed! Total time execution: 0:14:18.509346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_all_datasets(interval=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZdxG3HSuJaW5",
   "metadata": {
    "id": "ZdxG3HSuJaW5"
   },
   "source": [
    "## Downloading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BX9aIckkJXEn",
   "metadata": {
    "id": "BX9aIckkJXEn"
   },
   "outputs": [],
   "source": [
    "!zip -r datasets.zip /content/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "Kfaho2OtJX6q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1627644705631,
     "user": {
      "displayName": "Carlos Gil",
      "photoUrl": "",
      "userId": "12309568779801871703"
     },
     "user_tz": -120
    },
    "id": "Kfaho2OtJX6q",
    "outputId": "cd29b553-c2c6-40d4-b38c-01528da13b93",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_dbc7563f-ad41-4ed7-87ac-30d27be017eb\", \"datasets.zip\", 15487115)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download(\"datasets.zip\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "generate_dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
