{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model_converter.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"code","metadata":{"id":"3TrxYIae5nNa"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","from google.colab import files\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import math\n","import glob\n","import os\n","!apt-get update && apt-get -qq install xxd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oivknFW6XJy"},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wy80qj0XVYyi"},"source":["## Generate a TensorFlow Lite Model\n","\n","Convert the frozen graph into a TensorFlow Lite model, which is fully quantized for use with embedded devices. The following cell will also print the model size."]},{"cell_type":"code","metadata":{"id":"9r7byq5k0WY4","executionInfo":{"status":"ok","timestamp":1627907000623,"user_tz":-120,"elapsed":433,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["MODELS_DIR = 'models'\n","if not os.path.exists(MODELS_DIR):\n","    os.mkdir(MODELS_DIR)\n","\n","SAVED_MODEL_FILENAME = os.path.join(MODELS_DIR, \"mhr\")\n","FLOAT_TFL_MODEL_FILENAME = os.path.join(MODELS_DIR, \"mhr_float.tfl\")\n","QUANTIZED_TFL_MODEL_FILENAME = os.path.join(MODELS_DIR, \"mhr.tfl\")\n","TFL_CC_MODEL_FILENAME = os.path.join(MODELS_DIR, \"mhr.cc\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"x8yQ8Oghz6Qw"},"source":["!wget https://raw.githubusercontent.com/cargilgar/Smart-Alarm-using-tinyML/main/data/models/mhr_saved_model.pb\n","!wget https://raw.githubusercontent.com/cargilgar/Smart-Alarm-using-tinyML/main/data/models/keras_metadata.pb\n","\n","!mkdir ./models/mhr/\n","\n","!mv mhr_saved_model.pb saved_model.pb\n","\n","!mv *.pb ./models/mhr/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t-hU8aU24gbL"},"source":["# MODELS_DIR = 'models'\n","# if not os.path.exists(MODELS_DIR):\n","#     os.mkdir(MODELS_DIR)\n","\n","# SAVED_MODEL_FILENAME = os.path.join(MODELS_DIR, \"mhr\")\n","# FLOAT_TFL_MODEL_FILENAME = os.path.join(MODELS_DIR, \"mhr_float.tfl\")\n","# QUANTIZED_TFL_MODEL_FILENAME = os.path.join(MODELS_DIR, \"mhr.tfl\")\n","# TFL_CC_MODEL_FILENAME = os.path.join(MODELS_DIR, \"mhr.cc\")\n","\n","# load model\n","# !wget model.pb SAVED_MODEL_FILENAME\n","\n","\n","converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_FILENAME)\n","model_no_quant_tflite = converter.convert()\n","\n","# Save the model to disk\n","open(FLOAT_TFL_MODEL_FILENAME, \"wb\").write(model_no_quant_tflite)\n","\n","# Set the optimization flag.\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","\n","# Enforce integer only quantization\n","converter.inference_input_type = tf.int8\n","converter.inference_output_type = tf.int8\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","\n","DATA_SAMPLE_SIZE = 100\n","\n","def representative_data_gen():\n","    df_sample = np.array(df.sample(DATA_SAMPLE_SIZE), dtype=np.float32)\n","\n","    for row in range(DATA_SAMPLE_SIZE):\n","        ret = df_sample[row][:-1]  # take all the columns except for labels\n","        yield list(ret.reshape(1, ret.shape[0]))  # the input must be 1 \n","\n","\n","converter.representative_dataset = representative_data_gen\n","\n","model_tflite = converter.convert()\n","\n","# Save the model to disk\n","open(QUANTIZED_TFL_MODEL_FILENAME, \"wb\").write(model_tflite)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xw2vpCwJ_oGl"},"source":["# command line\n","# tflite_convert --output_file=model.tflite --saved_model_dir=/tmp/saved_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zxV0oM8FVoww"},"source":["Compare the sizes of the Tensorflow, TensorFlow Lite and Quantized TensorFlow Lite models."]},{"cell_type":"code","metadata":{"id":"NYrF0EA6zvOI"},"source":["def get_dir_size(dir):\n","    size = 0\n","    for f in os.scandir(dir):\n","        if f.is_file():\n","            size += f.stat().st_size\n","    elif f.is_dir():\n","        size += get_dir_size(f.path)\n","    return size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NTjGMU8BPpoz"},"source":["# Calculate size\n","size_tf = get_dir_size(SAVED_MODEL_FILENAME)\n","size_no_quant_tflite = os.path.getsize(FLOAT_TFL_MODEL_FILENAME)\n","size_tflite = os.path.getsize(QUANTIZED_TFL_MODEL_FILENAME)\n","\n","# Compare size\n","pd.DataFrame.from_records(\n","    [[\"TensorFlow\", f\"{size_tf} bytes\", \"\"],\n","     [\"TensorFlow Lite\", f\"{size_no_quant_tflite} bytes \", f\"(reduced by {size_tf - size_no_quant_tflite} bytes)\"],\n","     [\"TensorFlow Lite Quantized\", f\"{size_tflite} bytes\", f\"(reduced by {size_no_quant_tflite - size_tflite} bytes)\"]],\n","     columns = [\"Model\", \"Size\", \"\"])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v7D6JWbmzvOO"},"source":["The generated `quantized model` depicts a **x4** times reduction in size compared to the `float model` (orginial version).\n","\n","So far so good. Let's see how much the penalty has been for the accuracy metric."]},{"cell_type":"markdown","metadata":{"id":"4z2MCkkVzvOS"},"source":["### Testing the accuracy after Quantization\n","\n","Verify that the model we've exported is still accurate, using the TF Lite Python API and our test set."]},{"cell_type":"code","metadata":{"id":"dP83qjhTzvOT"},"source":["# Helper function to run inference\n","def run_tflite_inference_testSet(tflite_model_path, model_type=\"Float\"):\n","    \n","    # --- Load test data    \n","    SAMPLES = 10000\n","    test_data = np.array(test_df.sample(SAMPLES)).reshape(SAMPLES, test_df.shape[1])\n","    \n","    test_samples = np.zeros((SAMPLES, test_data.shape[1]-1), dtype=np.float32)\n","    test_labels = np.zeros((SAMPLES, 1), dtype=np.float32)\n","    \n","    for row in range(SAMPLES):\n","        test_labels[row] = test_data[row][-1:]\n","        test_samples[row] = test_data[row][:-1]\n","\n","    test_samples = np.expand_dims(test_samples, axis=1).astype(np.float32)\n","    \n","    # --- Initialize the interpreter\n","    interpreter = tf.lite.Interpreter(tflite_model_path)\n","    interpreter.allocate_tensors()\n","    input_details = interpreter.get_input_details()[0]\n","    output_details = interpreter.get_output_details()[0]\n","    \n","    \n","    # --- For quantized models, manually quantize the input data from float to integer    \n","    if model_type == \"Quantized\":\n","        input_scale, input_zero_point = input_details[\"quantization\"]\n","        test_samples = test_samples / input_scale + input_zero_point\n","        test_samples = test_samples.astype(input_details[\"dtype\"])\n","\n","    \n","    # --- Evaluate the predictions    \n","    correct_predictions = 0\n","    for i in range(len(test_samples)):\n","        interpreter.set_tensor(input_details[\"index\"], test_samples[i])\n","        interpreter.invoke()\n","        output = interpreter.get_tensor(output_details[\"index\"])[0]\n","        top_prediction = output.argmax()\n","        correct_predictions += (top_prediction == test_labels[i])\n","\n","    print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n","        model_type, (correct_predictions * 100) / len(test_samples), len(test_samples)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2v1l1ABpzvOV"},"source":["# Compute float model accuracy\n","run_tflite_inference_testSet(FLOAT_TFL_MODEL_FILENAME)\n","\n","# Compute quantized model accuracy\n","run_tflite_inference_testSet(QUANTIZED_TFL_MODEL_FILENAME, model_type='Quantized')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d8R-nlBYYjrP"},"source":["## Generate TensorFlow Lite for Microcontrollers Model\n","Using the ```xxd``` tool to convert the ```.tflite``` TensorFlow Lite quantized model into a ```.cc``` C source file, that can be loaded by TensorFlow Lite for Microcontrollers on the Arduino."]},{"cell_type":"code","metadata":{"id":"mrvnEJLfR8KU"},"source":["# Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model\n","!xxd -i {QUANTIZED_TFL_MODEL_FILENAME} > {TFL_CC_MODEL_FILENAME}\n","\n","# Update variable names\n","REPLACE_TEXT = QUANTIZED_TFL_MODEL_FILENAME.replace('/', '_').replace('.', '_')\n","!sed -i 's/'{REPLACE_TEXT}'/g_magic_wand_model_data/g' {TFL_CC_MODEL_FILENAME}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XfbwHwBjZL3U"},"source":["That's it! You've successfully converted your TensorFlow Lite model into a TensorFlow Lite for Microcontrollers model! Run the cell below to print out its contents which we'll need for our next step, deploying the model using the Arudino IDE!"]},{"cell_type":"code","metadata":{"id":"oazLUtBqWzdJ"},"source":["# Print the C source file\n","!cat {TFL_CC_MODEL_FILENAME}"],"execution_count":null,"outputs":[]}]}