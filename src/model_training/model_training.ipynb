{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copia de Model_Training.ipynb","provenance":[{"file_id":"1mCv8VG32cBpHwEvs6xJ63vWNyYsjYX_x","timestamp":1622746751991}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyO1Piz2azibncg0d/SXiZD3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"5iB8fV6iWeQs","executionInfo":{"status":"ok","timestamp":1622746442148,"user_tz":-120,"elapsed":2000,"user":{"displayName":"Daniel Moreno","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN-vX7mNsnTms9a2ZACAxsu0C62vmQUNoWZz60MP0=s64","userId":"03239533346995134647"}}},"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import os\n","import re\n","from scipy import stats\n","from keras.models import Sequential\n","from keras.callbacks import History \n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from keras.layers import Dense\n","from keras.optimizers import SGD\n","from keras.utils.np_utils import to_categorical\n","#from matplotlib import pyplot"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1R0wTJdcZzA9"},"source":["# Load Dataset\n","\n","Load Dataset, count Rows and Columns"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uge8UaRLVvuT","executionInfo":{"status":"ok","timestamp":1622746442149,"user_tz":-120,"elapsed":12,"user":{"displayName":"Daniel Moreno","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN-vX7mNsnTms9a2ZACAxsu0C62vmQUNoWZz60MP0=s64","userId":"03239533346995134647"}},"outputId":"42a39b15-8e11-4e9c-a774-fc5b189698bc"},"source":["data_path = os.path.join(os.getcwd(), \"dataset/\")\n","data_list = sorted(os.listdir(data_path))\n","data_list[0]\n","\n","\n","subject_csv = pd.read_csv(os.path.join(data_path, data_list[0]), delimiter=',')\n","subject_csv\n","\n","#Rows and Columns\n","total_rows=len(subject_csv.axes[0]) #===> Axes of 0 is for a row\n","total_cols=len(subject_csv.axes[1]) #===> Axes of 1 is for a column\n","print(\"Number of Rows: \"+str(total_rows))\n","print(\"Number of Columns: \"+str(total_cols))\n","\n","#subject_csv"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Number of Rows: 1559\n","Number of Columns: 6\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"00jGhBmxqSXE"},"source":["# Labels\n","stage (0-5, wake = 0, N1 = 1, N2 = 2, N3 = 3, REM = 5)"]},{"cell_type":"code","metadata":{"id":"uw3S8hjGdj0T","executionInfo":{"status":"ok","timestamp":1622746442150,"user_tz":-120,"elapsed":10,"user":{"displayName":"Daniel Moreno","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN-vX7mNsnTms9a2ZACAxsu0C62vmQUNoWZz60MP0=s64","userId":"03239533346995134647"}}},"source":["#Show labels\n","# Same labels will be reused throughout the program\n","\n","subject_csv['Labels'].describe()\n","#subject_csv.hist('Heart Rate')\n","#subject_csv.describe()\n","\n","subject_csv['Labels'] = subject_csv['Labels'].map({0:0,1:1,2:1,3:1,5:2,},na_action=None)\n","\n","# Wake --> 0\n","# NREM --> 1\n","# REM --> 2\n","\n","#Delete non-labeled Rows\n","subject_csv.dropna(inplace=True)\n","\n","#Not labeled values --> NaN\n","\n","#subject_csv"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oC7ObCbBZ_1-"},"source":["# Split Dataset\n","Train, Validation and Test\n","\n","Split the data\n","We'll use a (70%, 20%, 10%) split for the training, validation, and test sets. Note the data is not being randomly shuffled before splitting. This is for two reasons.\n","\n","It ensures that chopping the data into windows of consecutive samples is still possible.\n","It ensures that the validation/test results are more realistic, being evaluated on data collected after the model was trained.\n","\n","***ANOTHER SPLITING OPTION WOULD BE TO SEPARATE USERS (Crear nueva columna con nombre usuario?? O manejar cada CSV por separado??)***\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"DUcpsQLZaKOE","executionInfo":{"status":"ok","timestamp":1622746442150,"user_tz":-120,"elapsed":10,"user":{"displayName":"Daniel Moreno","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN-vX7mNsnTms9a2ZACAxsu0C62vmQUNoWZz60MP0=s64","userId":"03239533346995134647"}}},"source":["column_indices = {name: i for i, name in enumerate(subject_csv.columns)}\n","\n","PERCENTAGE_TRAIN = 0.7\n","PERCENTAGE_VALIDATION = 0.2\n","\n","n = len(subject_csv)\n","train_subject_csv = subject_csv[0:int(n*PERCENTAGE_TRAIN)]\n","val_subject_csv = subject_csv[int(n*PERCENTAGE_TRAIN):int(n*(PERCENTAGE_VALIDATION + PERCENTAGE_TRAIN))]\n","test_subject_csv = subject_csv[int(n*(PERCENTAGE_VALIDATION + PERCENTAGE_TRAIN)):]\n","\n","num_features = subject_csv.shape[1]\n","#test_subject_csv"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VwnxK53835FY"},"source":["# Normalize Training Data\n","Next, we need to normalize our features within our training data. Of course there are various ways on how to normalize. Please keep in mind that you use the same normalization algorithm later when feeding new data into your neural network. Otherwise your preditions will be off. On top of the normalization we will also apply rounding to the three features."]},{"cell_type":"code","metadata":{"id":"bOd1dXMi2N4_","executionInfo":{"status":"ok","timestamp":1622746442151,"user_tz":-120,"elapsed":11,"user":{"displayName":"Daniel Moreno","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN-vX7mNsnTms9a2ZACAxsu0C62vmQUNoWZz60MP0=s64","userId":"03239533346995134647"}}},"source":["# Normalize features for training data set (values between 0 and 1)***\n","# Surpress warning for next 3 operation\n","pd.options.mode.chained_assignment = None  # default='warn'\n","train_subject_csv['X'] = train_subject_csv['X'] / train_subject_csv['X'].max()\n","train_subject_csv['Y'] = train_subject_csv['Y'] / train_subject_csv['Y'].max()\n","train_subject_csv['Z'] = train_subject_csv['Z'] / train_subject_csv['Z'].max()\n","train_subject_csv['Heart Rate'] = train_subject_csv['Heart Rate'] / train_subject_csv['Heart Rate'].max()\n","\n","# Round numbers (4 decimals)\n","train_subject_csv = train_subject_csv.round({'X': 4, 'Y': 4, 'Z': 4, 'Heart Rate': 4})\n","\n","#train_subject_csv"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aYGO3D4b7btV"},"source":["# Reshape Data into Segments and Prepare for Keras\n","The data contained in the dataframe is not ready yet to be fed into a neural network. Therefore we need to reshape it. Let’s create another function for this called “create_segments_and_labels”. This function will take in the dataframe and the label names (the constant that we have defined at the beginning) as well as the length of each record. In our case, let’s go with 80 steps (see constant defined earlier). Taking into consideration the 20 Hz sampling rate, this equals to 4 second time intervals (calculation: 0.05 * 80 = 4). Besides reshaping the data, the function will also separate the features (x-acceleration, y-acceleration, z-acceleration) and the labels (associated activity). https://towardsdatascience.com/human-activity-recognition-har-tutorial-with-keras-and-core-ml-part-1-8c05e365dfa0"]},{"cell_type":"code","metadata":{"id":"2jOMBw867eW2","executionInfo":{"status":"ok","timestamp":1622746442151,"user_tz":-120,"elapsed":11,"user":{"displayName":"Daniel Moreno","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN-vX7mNsnTms9a2ZACAxsu0C62vmQUNoWZz60MP0=s64","userId":"03239533346995134647"}}},"source":["def create_segments_and_labels(subject_csv,labels):\n","\n","    labels = subject_csv[labels]\n","    #segments = subject_csv[['Z','Heart Rate']]\n","    segments = subject_csv[['X','Y','Z','Heart Rate']] #All features\n","\n","    return segments, labels"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"ODyG2FBGAGd6","executionInfo":{"status":"ok","timestamp":1622746442151,"user_tz":-120,"elapsed":9,"user":{"displayName":"Daniel Moreno","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN-vX7mNsnTms9a2ZACAxsu0C62vmQUNoWZz60MP0=s64","userId":"03239533346995134647"}}},"source":["# x_train --> Features\n","# y_train --> Labels\n","   \n","\n","x_train, y_train = create_segments_and_labels(train_subject_csv,'Labels')\n","x_val, y_val = create_segments_and_labels(val_subject_csv,'Labels')\n","x_test, y_test = create_segments_and_labels(test_subject_csv,'Labels')\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_6uxZqJTOTx","executionInfo":{"status":"ok","timestamp":1622746442151,"user_tz":-120,"elapsed":8,"user":{"displayName":"Daniel Moreno","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN-vX7mNsnTms9a2ZACAxsu0C62vmQUNoWZz60MP0=s64","userId":"03239533346995134647"}},"outputId":"acd62673-a363-4309-8b80-c08c5dd7e609"},"source":["print(x_train.shape[0], 'training samples')\n","print('x_train shape: ', x_train.shape)\n","print('y_train shape: ', y_train.shape)\n","x_train.shape[0]\n","#x_train"],"execution_count":8,"outputs":[{"output_type":"stream","text":["1091 training samples\n","x_train shape:  (1091, 4)\n","y_train shape:  (1091, 3)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["1091"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"mCXw2xaUJQ_l"},"source":["# Create Deep Neural Network Model in Keras\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-DyMELaTeUqD","executionInfo":{"status":"ok","timestamp":1622746442842,"user_tz":-120,"elapsed":697,"user":{"displayName":"Daniel Moreno","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN-vX7mNsnTms9a2ZACAxsu0C62vmQUNoWZz60MP0=s64","userId":"03239533346995134647"}},"outputId":"086737f3-d8f2-4cc8-db36-fbea6a8c6fee"},"source":["# define the keras model\n","num_classes = 3\n","\n","#Needed to categorical_crossentropy loss function\n","#y_train = to_categorical(y_train)\n","#y_val = to_categorical(y_val)\n","#y_test = to_categorical(y_test)\n","\n","model = Sequential()\n","model.add(Dense(60, input_dim= x_train.shape[1], activation='softmax'))\n","model.add(Dense(num_classes, activation='softmax'))#Last layer corresponds with the number of possible outputs\n","\n","# compile the keras model\n","opt = SGD(learning_rate=0.01, momentum=0.9)\n","\n","model.compile(loss= 'sparse_categorical_crossentropy', optimizer= opt, metrics=['accuracy'])\n","model.summary()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense (Dense)                (None, 60)                300       \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 3)                 183       \n","=================================================================\n","Total params: 483\n","Trainable params: 483\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z7L-f61Kr9Qm"},"source":["# Train Model"]},{"cell_type":"code","metadata":{"id":"O1byX-NigRhU"},"source":["# fit the keras model on the dataset\n","history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=36, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b7wDeVitsAE-"},"source":["# Evaluate Model on Test Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":849},"id":"6Ew4p3YooK3m","executionInfo":{"status":"error","timestamp":1622746474187,"user_tz":-120,"elapsed":452,"user":{"displayName":"Daniel Moreno","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN-vX7mNsnTms9a2ZACAxsu0C62vmQUNoWZz60MP0=s64","userId":"03239533346995134647"}},"outputId":"515d541e-573f-4680-8c93-302b4b6fd195"},"source":["# evaluate the keras model\n","\n","test_loss, test_acc = model.evaluate(x_test, y_test)\n","\n","print(\"Test accuracy\", test_acc)\n","print(\"Test loss\", test_loss)"],"execution_count":11,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-583a1fa0c7c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# evaluate the keras model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1464\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1465\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m-> 3022\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3439\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3440\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3441\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3363\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3364\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1298 test_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1282 run_step  *\n        outputs = model.test_step(data)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1243 test_step  *\n        self.compiled_loss(\n    /usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py:201 __call__  *\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.7/dist-packages/keras/losses.py:142 __call__  *\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.7/dist-packages/keras/losses.py:246 call  *\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/losses.py:1631 categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/backend.py:4827 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py:1161 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n"]}]},{"cell_type":"code","metadata":{"id":"EfMvpYkxYYU4","executionInfo":{"status":"aborted","timestamp":1622746474184,"user_tz":-120,"elapsed":11,"user":{"displayName":"Daniel Moreno","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN-vX7mNsnTms9a2ZACAxsu0C62vmQUNoWZz60MP0=s64","userId":"03239533346995134647"}}},"source":["# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7uAf9jsdJaEG"},"source":["\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6G7kPrQWKL88"},"source":["Shuffle the training set because we will be using the validation_split option later when training."]},{"cell_type":"code","metadata":{"id":"APAAU9zoKGCt","executionInfo":{"status":"aborted","timestamp":1622746474185,"user_tz":-120,"elapsed":11,"user":{"displayName":"Daniel Moreno","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN-vX7mNsnTms9a2ZACAxsu0C62vmQUNoWZz60MP0=s64","userId":"03239533346995134647"}}},"source":["'''idx = np.random.permutation(len(x_train))\n","x_train = x_train[idx]\n","y_train = y_train[idx]"],"execution_count":null,"outputs":[]}]}