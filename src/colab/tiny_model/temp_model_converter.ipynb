{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"temp_model_converter.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"3TrxYIae5nNa"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","from google.colab import files\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import math\n","import glob\n","import os\n","!apt-get update && apt-get -qq install xxd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oivknFW6XJy"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wy80qj0XVYyi"},"source":["## Generate a TensorFlow Lite Model\n","\n","Convert the frozen graph into a TensorFlow Lite model, which is fully quantized for use with embedded devices. The following cell will also print the model size."]},{"cell_type":"code","metadata":{"id":"t-hU8aU24gbL"},"source":["MODELS_DIR = 'models'\n","if not os.path.exists(MODELS_DIR):\n","  os.mkdir(MODELS_DIR)\n","\n","SAVED_MODEL_FILENAME = os.path.join(MODELS_DIR, \"mhr\")\n","FLOAT_TFL_MODEL_FILENAME = os.path.join(MODELS_DIR, \"mhr_float.tfl\")\n","QUANTIZED_TFL_MODEL_FILENAME = os.path.join(MODELS_DIR, \"mhr.tfl\")\n","TFL_CC_MODEL_FILENAME = os.path.join(MODELS_DIR, \"mhr.cc\")\n","\n","#save trained model\n","model.save(SAVED_MODEL_FILENAME)\n","\n","converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_FILENAME)\n","model_no_quant_tflite = converter.convert()\n","\n","# Save the model to disk\n","open(FLOAT_TFL_MODEL_FILENAME, \"wb\").write(model_no_quant_tflite)\n","\n","# Set the optimization flag.\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","# Enforce integer only quantization\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","converter.inference_input_type = tf.int8\n","converter.inference_output_type = tf.int8\n","\n","model_tflite = converter.convert()\n","\n","# Save the model to disk\n","open(QUANTIZED_TFL_MODEL_FILENAME, \"wb\").write(model_tflite)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xw2vpCwJ_oGl"},"source":["# command line\n","# tflite_convert --output_file=model.tflite --saved_model_dir=/tmp/saved_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zxV0oM8FVoww"},"source":["Compare the sizes of the Tensorflow, TensorFlow Lite and Quantized TensorFlow Lite models."]},{"cell_type":"code","metadata":{"id":"NTjGMU8BPpoz"},"source":["def get_dir_size(dir):\n","  size = 0\n","  for f in os.scandir(dir):\n","    if f.is_file():\n","      size += f.stat().st_size\n","    elif f.is_dir():\n","      size += get_dir_size(f.path)\n","  return size\n","\n","# Calculate size\n","size_tf = get_dir_size(SAVED_MODEL_FILENAME)\n","size_no_quant_tflite = os.path.getsize(FLOAT_TFL_MODEL_FILENAME)\n","size_tflite = os.path.getsize(QUANTIZED_TFL_MODEL_FILENAME)\n","\n","# Compare size\n","pd.DataFrame.from_records(\n","    [[\"TensorFlow\", f\"{size_tf} bytes\", \"\"],\n","     [\"TensorFlow Lite\", f\"{size_no_quant_tflite} bytes \", f\"(reduced by {size_tf - size_no_quant_tflite} bytes)\"],\n","     [\"TensorFlow Lite Quantized\", f\"{size_tflite} bytes\", f\"(reduced by {size_no_quant_tflite - size_tflite} bytes)\"]],\n","     columns = [\"Model\", \"Size\", \"\"], index=\"Model\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d8R-nlBYYjrP"},"source":["## Generate a TensorFlow Lite for Microcontrollers Model\n","To convert the TensorFlow Lite quantized model into a C source file that can be loaded by TensorFlow Lite for Microcontrollers on Arduino we simply need to use the ```xxd``` tool to convert the ```.tflite``` file into a ```.cc``` file."]},{"cell_type":"code","metadata":{"id":"mrvnEJLfR8KU"},"source":["# Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model\n","!xxd -i {QUANTIZED_TFL_MODEL_FILENAME} > {TFL_CC_MODEL_FILENAME}\n","\n","# Update variable names\n","REPLACE_TEXT = QUANTIZED_TFL_MODEL_FILENAME.replace('/', '_').replace('.', '_')\n","!sed -i 's/'{REPLACE_TEXT}'/g_magic_wand_model_data/g' {TFL_CC_MODEL_FILENAME}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XfbwHwBjZL3U"},"source":["That's it! You've successfully converted your TensorFlow Lite model into a TensorFlow Lite for Microcontrollers model! Run the cell below to print out its contents which we'll need for our next step, deploying the model using the Arudino IDE!"]},{"cell_type":"code","metadata":{"id":"oazLUtBqWzdJ"},"source":["# Print the C source file\n","!cat {TFL_CC_MODEL_FILENAME}"],"execution_count":null,"outputs":[]}]}