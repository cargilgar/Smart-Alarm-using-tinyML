{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"name":"generate_dataset.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"code","metadata":{"id":"1pH40itHTdra"},"source":["# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"id":"1pH40itHTdra","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"88Ve2wbXTfNL"},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"id":"88Ve2wbXTfNL","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PS7-hk5PTbUU","scrolled":false},"source":["!sudo apt-get install wget\n","\n","!wget -r -nv -N -c -np https://physionet.org/files/sleep-accel/1.0.0/\n","\n","!mkdir ./download\n","\n","!mv ./physionet.org/files/sleep-accel/1.0.0/* download\n","\n","!rm -r ./physionet.org/\n","\n","!find ./download -name \"*.html\" -type f -delete"],"id":"PS7-hk5PTbUU","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"stylish-wages"},"source":["Steps:\n","\n","- Download and unzip the dataset\n","- Load the files\n","- Pre-process the loaded files (crop to keep the part of interest)\n","- Merge files from each user selecting a specific window time frame.\n","- Export the resulting file to `.csv`.\n","- Repeat the process for all the users."],"id":"stylish-wages"},{"cell_type":"code","metadata":{"id":"forbidden-forty","executionInfo":{"status":"ok","timestamp":1622738111720,"user_tz":-120,"elapsed":39,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["import pandas as pd\n","import numpy as np\n","import os\n","import re"],"id":"forbidden-forty","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"qmXPkoZWyyEm","executionInfo":{"status":"ok","timestamp":1622738111721,"user_tz":-120,"elapsed":27,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["from enum import Enum\n","\n","class Error(Enum):\n","    match_number_users = \"[Error]: number of users in list does not match\"\n","    match_user_id = \"[Error]: user id does not match between lists\"\n","    match_length_arrays = \"[Error]: the length of the lists does not match\"\n","    match_index = \"[Error]: indexes are mismatched\"\n","    generic_error = \"[Error]\""],"id":"qmXPkoZWyyEm","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"expanded-flood","executionInfo":{"status":"ok","timestamp":1622743032253,"user_tz":-120,"elapsed":206,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["# Preparing paths\n","# data_path = \"C:\\dev\\DATA\\MRH\"\n","data_path = os.path.join(os.getcwd(), \"dataset/\")\n","\n","motion_path = os.path.join(data_path, \"motion\")\n","\n","heart_rate_path = os.path.join(data_path, \"heart_rate\")\n","\n","labels_path = os.path.join(data_path, \"labels\")\n","\n","# Obtaining ordered lists with all users\n","motion_list = sorted(os.listdir(motion_path))\n","heart_rate_list = sorted(os.listdir(heart_rate_path))\n","labels_list = sorted(os.listdir(labels_path))\n","\n","# Checking that we have data of the 31 users in all the lists created\n","assert len(motion_list) == 31, Error.match_number_users.value\n","assert len(heart_rate_list) == 31, Error.match_number_users.value\n","assert len(labels_list) == 31, Error.match_number_users.value\n","\n","# Checking that the user ids match in order accross the three lists\n","for item in range(len(motion_list)):\n","    user_motion_id = re.search(\"\\d*\", motion_list[item])\n","    user_heart_rate_id = re.search(\"\\d*\", heart_rate_list[item])\n","    user_labels_id = re.search(\"\\d*\", labels_list[item])\n","\n","    assert user_motion_id.group(0) == user_heart_rate_id.group(0), Error.match_user_id.value\n","    assert user_motion_id.group(0) == user_labels_id.group(0), Error.match_user_id.value\n"],"id":"expanded-flood","execution_count":92,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hungry-worker","executionInfo":{"status":"ok","timestamp":1622743044012,"user_tz":-120,"elapsed":310,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}},"outputId":"eeb4de39-5669-494d-b78d-2fb56f7fe40d"},"source":["motion_list[0], heart_rate_list[0], labels_list[0]"],"id":"hungry-worker","execution_count":93,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('1066528_acceleration.txt',\n"," '1066528_heartrate.txt',\n"," '1066528_labeled_sleep.txt')"]},"metadata":{"tags":[]},"execution_count":93}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"known-welding","executionInfo":{"status":"ok","timestamp":1622743082430,"user_tz":-120,"elapsed":11494,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}},"outputId":"f3c3c4a9-2ea0-427c-ebbd-745362a61d38"},"source":["user_1_motion = np.loadtxt(os.path.join(motion_path, motion_list[0]))\n","\n","user_1_motion"],"id":"known-welding","execution_count":94,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-2.16848465e+04,  7.08010000e-03,  6.40900000e-04,\n","        -9.87594600e-01],\n","       [-2.16848171e+04,  4.15040000e-03,  6.25600000e-04,\n","        -9.90554800e-01],\n","       [-2.16848079e+04,  4.15040000e-03,  1.11390000e-03,\n","        -9.90081800e-01],\n","       ...,\n","       [ 2.86265419e+04, -5.52734400e-01, -2.99988000e-02,\n","        -8.10440100e-01],\n","       [ 2.86265428e+04, -5.53710900e-01, -3.05023000e-02,\n","        -8.11431900e-01],\n","       [ 2.86265436e+04, -5.54718000e-01, -2.99988000e-02,\n","        -8.09021000e-01]])"]},"metadata":{"tags":[]},"execution_count":94}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"terminal-conviction","executionInfo":{"status":"ok","timestamp":1622743082432,"user_tz":-120,"elapsed":31,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}},"outputId":"b4a5504a-6c3e-427b-f31f-e1d0b2cd2625"},"source":["user_1_heart_rate = np.loadtxt(os.path.join(heart_rate_path, heart_rate_list[0]), delimiter=',')\n","\n","user_1_heart_rate"],"id":"terminal-conviction","execution_count":95,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-3.55241740e+05,  8.60000000e+01],\n","       [-3.51407999e+05,  6.70000000e+01],\n","       [-3.51277368e+05,  1.41000000e+02],\n","       ...,\n","       [ 2.91101643e+04,  7.50000000e+01],\n","       [ 3.43346538e+04,  8.10000000e+01],\n","       [ 3.44911535e+04,  6.50000000e+01]])"]},"metadata":{"tags":[]},"execution_count":95}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"random-conflict","executionInfo":{"status":"ok","timestamp":1622743082748,"user_tz":-120,"elapsed":340,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}},"outputId":"4c812de3-44ce-46d2-d415-bfb36dcb66f0"},"source":["user_1_labels = np.loadtxt(os.path.join(labels_path, labels_list[0]))\n","\n","user_1_labels"],"id":"random-conflict","execution_count":96,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[    0.,     0.],\n","       [   30.,     0.],\n","       [   60.,     0.],\n","       ...,\n","       [28470.,     0.],\n","       [28500.,     0.],\n","       [28530.,     0.]])"]},"metadata":{"tags":[]},"execution_count":96}]},{"cell_type":"code","metadata":{"id":"earlier-contractor","executionInfo":{"status":"ok","timestamp":1622743205389,"user_tz":-120,"elapsed":199,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["def generate_dataset(motion_user, heart_rate_user, labels_user, interval_peak=1, interval_epoch=30):\n","\n","    '''\n","    It accepts three filenames from one user to generate the dataset. Interval stands for the time in seconds of windowing.\n","    '''\n","    \n","    # --- Loading the txt files\n","    motion = np.loadtxt(motion_user)\n","    heart_rate = np.loadtxt(heart_rate_user, delimiter=',')\n","    labels = np.loadtxt(labels_user)\n","\n","    \n","    # --- Cropping to match the labelled list\n","    motion = crop_to_offset(motion, labels)    \n","    heart_rate = crop_to_offset(heart_rate, labels)\n","    \n","    # --- Pre-processing and merging\n","    motion = get_summary_count(motion, interval_peak, interval_epoch)\n","    \n","    merged_arrays = merge_arrays(motion, heart_rate, labels, interval_epoch)\n","    \n","    \n","    data_frame = pd.DataFrame(merged_arrays, columns=['Time', 'X', 'Y', 'Z', 'Heart Rate', 'Labels'])\n","  \n","  \n","    \n","    # OLD:\n","    # Extending smaller arrays to have the same size as the biggest array so as to be merged\n","    # It returns one dimensional array (time column skipped since it has been matched in the extending process)\n","    # heart_rate = extend_array(heart_rate, motion)\n","    # labels = extend_array(labels, motion)\n","    \n","    # Merging three arrays into one data frame\n","    # data_frame = pd.DataFrame(motion, columns=['Time', 'X', 'Y', 'Z', 'Heart Rate', 'Labels'])\n","    \n","    # heart_rate_column = pd.Series(heart_rate)\n","    # data_frame[\"Heart Rate\"] = heart_rate_column\n","    \n","    # labels_column = pd.Series(labels)\n","    # data_frame[\"Labels\"] = labels_column\n","    \n","    # return data_frame"],"id":"earlier-contractor","execution_count":103,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"touched-zimbabwe"},"source":["The raw data recorded with the Apple Watch (motion and heart rate) contains continiuous and uninterrumped measurements of one or more days, including the last night.\n","\n","Since the data corresponding to the last night underwent a proper labelling from the PSG results, it is necessary to crop the raw data only to that night (i.e. the list with labels). Anything else, will not be part of the generated dataset and will therefore be disregarded.\n","\n","This is handled by the function `crop_to_offset()`. This function carries out two tasks:\n","\n","1. It finds the last night measured within the array passed.\n","2. For the last night, it finds the boundaries corresponding to the start and end of the labelled list.\n","\n","Then, the function returns the indexes where the array needs to be sliced."],"id":"touched-zimbabwe"},{"cell_type":"code","metadata":{"id":"diagnostic-cooperative","executionInfo":{"status":"ok","timestamp":1622743200785,"user_tz":-120,"elapsed":211,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["def crop_to_offset(array_to_crop, array_ref):\n","    '''\n","    This function takes two arrays, the first is the one to be cropped and the second one the reference to where to crop.\n","    It returs a new array starting and ending where the indexes matched with the reference array.\n","    '''\n","\n","    start_index, end_index = 0, 0\n","    array_size = np.size(array_to_crop, 0)\n","    cropped_array = []\n","    \n","    # --- Find the boundaries corresponding to the labelled list\n","    first_item = array_ref[0][0]\n","    last_item = array_ref[-1][0]\n","    \n","    last_item_found = False\n","    \n","    for item in range(array_size - 1, -1, -1):        \n","        # find end index\n","        if not last_item_found:\n","            if array_to_crop[item][0] < last_item:\n","                end_index = item + 1\n","                last_item_found = True\n","        \n","        # find start index\n","        if array_to_crop[item][0] < first_item:\n","            start_index = item\n","            break  # No more iteration is needed after finding end_index and start_index.\n","    \n","    \n","    # return (start_index, end_index)\n","    cropped_array = array_to_crop[start_index:end_index]\n","        \n","    return cropped_array"],"id":"diagnostic-cooperative","execution_count":102,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hUP-MlGNFCWg"},"source":["In order to compress the vast amount of data gathered from the IMU sensor, some pre-processing is required. Tipically, these types of sensors record at a high frequencies resulting in hundreds of measurements every single second. Research suggest that changes among sleep cycles tend to occur gradually within a few minutes. This also applies to the shift between NREM and REM, which is the variable of interest that is the focus of this application.\n","\n","Therefore, the time resolution in the raw dataset is too accurate and the function `get_summary_count()` will handle this. All this function does is first finds the peak values within a user-defined time interval (we are most interested in peak values from the accelerometer that contribute to more valuable information) and sum all the spikes that take place within a window or time (epoch), also defined by the user.\n","\n","An example might be that the function finds the spike that occur in every second and then sums all the spikes found within a window of 15 seconds. This process then is repeated until completion."],"id":"hUP-MlGNFCWg"},{"cell_type":"code","metadata":{"id":"aSTcpB_WDxOV","executionInfo":{"status":"ok","timestamp":1622743241021,"user_tz":-120,"elapsed":211,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["def get_summary_count(array, peak_interval, sum_window):\n","    '''\n","    This function first finds the peak value for every peak_interval (s) of the passed array.\n","    It then sums all the peak values within sum_window (s) and adds the sum to a new array.\n","    It returns the resulting processed array back.\n","    '''\n","\n","    array_size = np.size(array, 0)\n","    peak_values_x, sum_peak_values_x = [], []\n","    peak_values_y, sum_peak_values_y = [], []\n","    peak_values_z, sum_peak_values_z = [], []\n","    \n","    # missing_indices = np.empty((0, 3), dtype=float)\n","    max_value_x, max_value_y, max_value_z = 0, 0, 0\n","    time_accumulate = []\n","    accumulate = 0\n","    last_interval = 0\n","    count = 0\n","\n","    for item in range(array_size):\n","        if (array[item][0] - last_interval) < peak_interval:\n","            if abs(array[item][1]) > abs(max_value_x):  # New peak value found at x\n","                max_value_x = abs(array[item][1])\n","\n","            if abs(array[item][2]) > abs(max_value_y):  # New peak value found at y\n","                max_value_y = abs(array[item][2])\n","\n","            if abs(array[item][3]) > abs(max_value_z):  # New peak value found at z\n","                max_value_z = abs(array[item][3])\n","\n","        # Gap found, do not continue with current window\n","        elif (array[item][0] - last_interval) > (peak_interval*1.5):\n","            # print(\"============================================\")\n","            # print(f\"[{array[item-1][0]}, {array[item][0]}]: {(array[item][0] - last_interval)}\")\n","            # print(\"============================================\")\n","                       \n","            # missing indices gets the time values of the gap between the missing data happened\n","            # new_interval = np.array([[array[item-1][0], array[item][0], (array[item][0] - array[item-1][0])]])\n","            # missing_indices = np.append(missing_indices, new_interval, axis=0)\n","            \n","            # update ,to the new time\n","            # print(\"acc: \", accumulate)\n","            # accumulate = np.around(accumulate + (np.around(array[item][0], decimals=0) - array[item-1][0]) + count, decimals=0)\n","            # accumulate = np.around(accumulate + (np.around(array[item][0], decimals=0) - last_interval) + count, decimals=0)\n","            accumulate = np.around(array[item][0], decimals=0)\n","            # print(\"acc after: \", accumulate)\n","            # print(\"count: \", count)\n","\n","            # reset for a new window time. Current unfinished window invalid.\n","            # if count < sum_window:              \n","            peak_values_x = []\n","            peak_values_y = []\n","            peak_values_z = []\n","            count = 0\n","            \n","            last_interval = np.floor(array[item][0])\n","            max_value_x = 0\n","            max_value_y = 0\n","            max_value_z = 0\n","            \n","        else:\n","            # end of peak interval\n","            peak_values_x.append(max_value_x)\n","            peak_values_y.append(max_value_y)\n","            peak_values_z.append(max_value_z)\n","\n","            # reset interval values and increment count\n","            last_interval = np.floor(array[item][0])\n","            max_value_x = 0\n","            max_value_y = 0\n","            max_value_z = 0\n","            count += 1\n","\n","        if count == sum_window:\n","            sum_peak_values_x.append(np.sum(peak_values_x))\n","            sum_peak_values_y.append(np.sum(peak_values_y))\n","            sum_peak_values_z.append(np.sum(peak_values_z))\n","            \n","            accumulate = np.around(accumulate + sum_window, decimals=0)\n","\n","            # print(\"====== acc:\", accumulate, \"val:\", np.around(array[item][0]), \"diff:\", (accumulate - array[item][0]))\n","            \n","            # Ensure that the current accumulated time matches the arrays' time\n","            assert abs(accumulate - np.around(array[item][0])) < 2, abs(accumulate - np.around(array[item][0]))\n","            # Error.match_index.value\n","\n","            # time_accumulate.append(time_accumulate[-1] + sum_window)\n","            time_accumulate.append(accumulate)\n","\n","            # reset for a new window time\n","            peak_values_x = []\n","            peak_values_y = []\n","            peak_values_z = []\n","            count = 0\n","       \n","\n","    \n","    assert len(time_accumulate) == len(sum_peak_values_x), Error.match_length_arrays.value\n","    assert len(time_accumulate) == len(sum_peak_values_y), Error.match_length_arrays.value\n","    assert len(time_accumulate) == len(sum_peak_values_z), Error.match_length_arrays.value\n","    \n","    # return time_accumulate\n","    \n","    return np.column_stack((time_accumulate, sum_peak_values_x, sum_peak_values_y, sum_peak_values_z))"],"id":"aSTcpB_WDxOV","execution_count":107,"outputs":[]},{"cell_type":"code","metadata":{"id":"6270mnO5fegg","executionInfo":{"status":"ok","timestamp":1622743091356,"user_tz":-120,"elapsed":1034,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["user_1_motion = crop_to_offset(user_1_motion, user_1_labels)\n","user_1_heart_rate = crop_to_offset(user_1_heart_rate, user_1_labels)\n","\n","# fname = \"cropped_\" + motion_list[1]\n","# np.savetxt(fname, user_1_motion, fmt='%.18e', delimiter=' ', newline='\\n', header='', footer='', comments='# ', encoding=None)\n","\n","# from google.colab import files\n","# files.download(fname) "],"id":"6270mnO5fegg","execution_count":97,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"arctic-spider","scrolled":true,"executionInfo":{"status":"ok","timestamp":1622743253131,"user_tz":-120,"elapsed":4015,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}},"outputId":"7ef49a8c-828d-4716-ba4b-7e5bb3fd3e8e"},"source":["# [time.prev, time.next, diff]\n","summary = get_summary_count(user_1_motion, 1, 15)\n","\n","summary"],"id":"arctic-spider","execution_count":109,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.50000000e+01, 6.12944040e+00, 6.76710490e+00, 1.20081636e+01],\n","       [3.00000000e+01, 6.11120610e+00, 6.92994680e+00, 1.19629060e+01],\n","       [4.50000000e+01, 6.09844980e+00, 6.90870630e+00, 1.19256594e+01],\n","       ...,\n","       [2.77820000e+04, 4.15765380e+00, 1.43766631e+01, 7.25236530e+00],\n","       [2.83750000e+04, 3.75987240e+00, 1.45188444e+01, 4.36595140e+00],\n","       [2.83900000e+04, 8.03883330e+00, 1.27173921e+01, 7.98275750e+00]])"]},"metadata":{"tags":[]},"execution_count":109}]},{"cell_type":"markdown","metadata":{"id":"growing-buyer"},"source":["Last step is merging the already pre-processed motion dataset with the heart rate and labels datasets corresponding to each user. \n","\n","For the heart rate dataset the following approach is taken. For each window from the motion time interval, gather all the heart rate values that were measured within that interval and get the mean value of all of them. If no heart rate value was measured within a window interval, the last known value will be taken. If there is a time gap bigger than the actual window's size, then discard that data up to the next available one.\n","\n","For the labels a simpler approach is taken. Since the time windows are meant to be always smaller or equal than the time interval of labels recorded, the label value will be unique. It can be either one that corresponds to that time window or the last one if missing in that interval. \n","\n","All the above-mentioned operations are carried out in the `merge_arrays()` method."],"id":"growing-buyer"},{"cell_type":"code","metadata":{"id":"invalid-difference","executionInfo":{"status":"ok","timestamp":1622743226771,"user_tz":-120,"elapsed":246,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["def merge_arrays(motion, heart_rate, labels, interval_epoch):\n","    # array_size = max(np.size(motion, 0), np.size(heart_rate, 0), np.size(labels, 0))\n","    # Checking that the biggest arrray to iterate through at this point is heart_rate\n","    # assert array_size == np.size(heart_rate, 0), Error.match_length_arrays\n","    \n","    # --- Heart Rate pre-processing\n","    \n","    heart_rate_size = np.size(heart_rate, 0)\n","    motion_size = np.size(motion, 0)\n","\n","    new_heart_rate = []\n","    heart_rate_acc = []\n","    \n","    inc = 0\n","        \n","    for i in range(heart_rate_size):        \n","\n","        if heart_rate[i][0] < motion[inc][0]:\n","            heart_rate_acc.append(heart_rate[i][1])\n","        else:\n","            # If time interval is bigger than the window time, take the latest value of HR\n","            if motion[inc][0] - motion[inc - 1][0] > interval_epoch:\n","                heart_rate_acc = [heart_rate[i][1]]\n","            \n","            # Append the mean off all values recorded\n","            new_heart_rate.append(sum(heart_rate_acc)/len(heart_rate_acc))\n","            # print(f\"[{motion[inc][0]}, {sum(heart_rate_acc)/len(heart_rate_acc)}]: \")\n","\n","            heart_rate_acc = [heart_rate[i][1]] # include first item that gave the condition too\n","            inc += 1    \n","\n","        # If the motion dattaset finishes but there is still heart rate dataset, dismiss the rest of the latter.\n","        if inc == motion_size:\n","            break\n","    \n","    # Removing exceding data of motion that is not available in heart rate.\n","    if len(new_heart_rate) < np.size(motion, 0):\n","        diff = abs(len(new_heart_rate) - np.size(motion, 0)) #3\n","        last_row = np.size(motion, 0)\n","        start = last_row - diff\n","\n","        subarray = np.arange(start, last_row)\n","\n","        motion = np.delete(motion, subarray, axis=0)\n","        \n","\n","    \n","    # --- Labels pre-processing\n","    \n","    # labels_size = np.size(labels, 0)\n","    motion_size = np.size(motion, 0)  # this might be new if deletion was needed before\n","    new_labels = []\n","    \n","    inc = 0\n","\n","    # print(labels_size)\n","    \n","    for j in range(motion_size):\n","        if motion[j][0] > labels[inc][0]:\n","            inc += 1\n","        \n","        new_labels.append(labels[inc][1])\n","\n","        # if inc == labels_size:\n","        #    break\n","    \n","    \n","    assert np.size(motion, 0) == len(new_heart_rate), Error.match_length_arrays.value\n","    assert np.size(motion, 0) == len(new_labels), Error.match_length_arrays.value\n","    \n","    return np.column_stack((motion, new_heart_rate, new_labels))"],"id":"invalid-difference","execution_count":105,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"0XbhkHDBvUAr","executionInfo":{"status":"ok","timestamp":1622743260057,"user_tz":-120,"elapsed":229,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}},"outputId":"8ab960c2-10d7-4f82-dab2-151e673538bb"},"source":["arr = merge_arrays(summary, user_1_heart_rate, user_1_labels, 15)\n","\n","df = pd.DataFrame(arr, columns=['Time', 'X', 'Y', 'Z', 'Heart Rate', 'Labels'])\n","\n","df"],"id":"0XbhkHDBvUAr","execution_count":110,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Time</th>\n","      <th>X</th>\n","      <th>Y</th>\n","      <th>Z</th>\n","      <th>Heart Rate</th>\n","      <th>Labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>15.0</td>\n","      <td>6.129440</td>\n","      <td>6.767105</td>\n","      <td>12.008164</td>\n","      <td>51.250000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>30.0</td>\n","      <td>6.111206</td>\n","      <td>6.929947</td>\n","      <td>11.962906</td>\n","      <td>52.666667</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>45.0</td>\n","      <td>6.098450</td>\n","      <td>6.908706</td>\n","      <td>11.925659</td>\n","      <td>52.333333</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>60.0</td>\n","      <td>6.101441</td>\n","      <td>6.911728</td>\n","      <td>11.935013</td>\n","      <td>49.333333</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>75.0</td>\n","      <td>6.093109</td>\n","      <td>6.915634</td>\n","      <td>11.933121</td>\n","      <td>51.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1554</th>\n","      <td>26057.0</td>\n","      <td>0.599274</td>\n","      <td>1.208099</td>\n","      <td>14.854935</td>\n","      <td>68.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1555</th>\n","      <td>26072.0</td>\n","      <td>4.737259</td>\n","      <td>8.527146</td>\n","      <td>12.957062</td>\n","      <td>68.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1556</th>\n","      <td>27725.0</td>\n","      <td>5.197281</td>\n","      <td>13.699905</td>\n","      <td>5.316589</td>\n","      <td>60.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1557</th>\n","      <td>27740.0</td>\n","      <td>5.081131</td>\n","      <td>13.254791</td>\n","      <td>5.919617</td>\n","      <td>60.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1558</th>\n","      <td>27755.0</td>\n","      <td>5.300781</td>\n","      <td>13.371521</td>\n","      <td>4.740448</td>\n","      <td>67.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1559 rows × 6 columns</p>\n","</div>"],"text/plain":["         Time         X          Y          Z  Heart Rate  Labels\n","0        15.0  6.129440   6.767105  12.008164   51.250000     0.0\n","1        30.0  6.111206   6.929947  11.962906   52.666667     0.0\n","2        45.0  6.098450   6.908706  11.925659   52.333333     0.0\n","3        60.0  6.101441   6.911728  11.935013   49.333333     0.0\n","4        75.0  6.093109   6.915634  11.933121   51.000000     0.0\n","...       ...       ...        ...        ...         ...     ...\n","1554  26057.0  0.599274   1.208099  14.854935   68.000000     0.0\n","1555  26072.0  4.737259   8.527146  12.957062   68.000000     0.0\n","1556  27725.0  5.197281  13.699905   5.316589   60.000000     0.0\n","1557  27740.0  5.081131  13.254791   5.919617   60.000000     0.0\n","1558  27755.0  5.300781  13.371521   4.740448   67.000000     0.0\n","\n","[1559 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":110}]},{"cell_type":"code","metadata":{"id":"vocational-aluminum","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622745693613,"user_tz":-120,"elapsed":608277,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}},"outputId":"c0bc5094-df0c-44f4-8604-ade8c49751ab"},"source":["import time\n","import datetime\n","\n","INTERVAL = 1\n","EPOCH = 15\n","time_acc = []\n","\n","for user in range(0, len(motion_list)):\n","    id_match = re.search(\"\\d*\", motion_list[user])\n","    user_id = id_match.group(0)\n","    \n","    start = time.time()\n","\n","    generate_dataset(os.path.join(motion_path, motion_list[user]), \n","                 os.path.join(heart_rate_path, heart_rate_list[user]), \n","                 os.path.join(labels_path, labels_list[user]), INTERVAL, EPOCH)\n","\n","\n","    time_acc.append(time.time() - start)\n","\n","    print('Dataset user id <{}> generated succesfully. Time: {:.0f} s'.format(user_id, time_acc[user]))\n","\n","\n","print(f'\\nProcess complete! Total time execution: {datetime.timedelta(seconds=sum(time_acc))}\\n')"],"id":"vocational-aluminum","execution_count":124,"outputs":[{"output_type":"stream","text":["Dataset user id <1066528> generated succesfully. Time: 16 s\n","Dataset user id <1360686> generated succesfully. Time: 18 s\n","Dataset user id <1449548> generated succesfully. Time: 22 s\n","Dataset user id <1455390> generated succesfully. Time: 19 s\n","Dataset user id <1818471> generated succesfully. Time: 20 s\n","Dataset user id <2598705> generated succesfully. Time: 19 s\n","Dataset user id <2638030> generated succesfully. Time: 24 s\n","Dataset user id <3509524> generated succesfully. Time: 13 s\n","Dataset user id <3997827> generated succesfully. Time: 22 s\n","Dataset user id <4018081> generated succesfully. Time: 13 s\n","Dataset user id <4314139> generated succesfully. Time: 19 s\n","Dataset user id <4426783> generated succesfully. Time: 22 s\n","Dataset user id <46343> generated succesfully. Time: 12 s\n","Dataset user id <5132496> generated succesfully. Time: 14 s\n","Dataset user id <5383425> generated succesfully. Time: 4 s\n","Dataset user id <5498603> generated succesfully. Time: 16 s\n","Dataset user id <5797046> generated succesfully. Time: 23 s\n","Dataset user id <6220552> generated succesfully. Time: 23 s\n","Dataset user id <759667> generated succesfully. Time: 14 s\n","Dataset user id <7749105> generated succesfully. Time: 5 s\n","Dataset user id <781756> generated succesfully. Time: 20 s\n","Dataset user id <8000685> generated succesfully. Time: 20 s\n","Dataset user id <8173033> generated succesfully. Time: 19 s\n","Dataset user id <8258170> generated succesfully. Time: 4 s\n","Dataset user id <844359> generated succesfully. Time: 18 s\n","Dataset user id <8530312> generated succesfully. Time: 20 s\n","Dataset user id <8686948> generated succesfully. Time: 19 s\n","Dataset user id <8692923> generated succesfully. Time: 22 s\n","Dataset user id <9106476> generated succesfully. Time: 20 s\n","Dataset user id <9618981> generated succesfully. Time: 90 s\n","Dataset user id <9961348> generated succesfully. Time: 20 s\n","\n","Process complete! Total time execution: 0:10:07.892873\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"opposite-command"},"source":["###### Saving sample"],"id":"opposite-command"},{"cell_type":"code","metadata":{"id":"requested-phone"},"source":["user_id = re.search(\"\\d*\", motion_list[0])\n","\n","fname = 'dataset_' + user_id.group(0) + \".csv\"\n","\n","user_1.to_csv(fname, index = False, header=True)"],"id":"requested-phone","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"academic-butler"},"source":["###### Testing DataFrame"],"id":"academic-butler"},{"cell_type":"code","metadata":{"id":"humanitarian-wilson","outputId":"b087e0bb-3498-445f-e339-c0e112c2982c"},"source":["user_1[user_1[\"Labels\"] > 4]"],"id":"humanitarian-wilson","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Time</th>\n","      <th>X</th>\n","      <th>Y</th>\n","      <th>Z</th>\n","      <th>Heart Rate</th>\n","      <th>Labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>458</th>\n","      <td>6885.0</td>\n","      <td>1.330994</td>\n","      <td>2.418594</td>\n","      <td>14.649231</td>\n","      <td>58.333333</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>459</th>\n","      <td>6900.0</td>\n","      <td>1.325516</td>\n","      <td>2.420059</td>\n","      <td>14.650604</td>\n","      <td>56.333333</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>460</th>\n","      <td>6915.0</td>\n","      <td>1.330429</td>\n","      <td>2.420959</td>\n","      <td>14.650238</td>\n","      <td>57.000000</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>461</th>\n","      <td>6930.0</td>\n","      <td>1.328049</td>\n","      <td>2.418518</td>\n","      <td>14.646271</td>\n","      <td>63.333333</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>462</th>\n","      <td>6945.0</td>\n","      <td>1.328033</td>\n","      <td>2.411621</td>\n","      <td>14.656220</td>\n","      <td>65.666667</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1480</th>\n","      <td>24947.0</td>\n","      <td>6.986115</td>\n","      <td>1.834229</td>\n","      <td>13.185440</td>\n","      <td>65.000000</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>1481</th>\n","      <td>24962.0</td>\n","      <td>6.983322</td>\n","      <td>1.860351</td>\n","      <td>13.181610</td>\n","      <td>61.000000</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>1482</th>\n","      <td>24977.0</td>\n","      <td>6.977661</td>\n","      <td>1.855469</td>\n","      <td>13.182022</td>\n","      <td>63.000000</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>1483</th>\n","      <td>24992.0</td>\n","      <td>6.976059</td>\n","      <td>1.863800</td>\n","      <td>13.195205</td>\n","      <td>67.500000</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>1484</th>\n","      <td>25007.0</td>\n","      <td>6.957992</td>\n","      <td>1.848099</td>\n","      <td>13.199081</td>\n","      <td>67.000000</td>\n","      <td>5.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>509 rows × 6 columns</p>\n","</div>"],"text/plain":["         Time         X         Y          Z  Heart Rate  Labels\n","458    6885.0  1.330994  2.418594  14.649231   58.333333     5.0\n","459    6900.0  1.325516  2.420059  14.650604   56.333333     5.0\n","460    6915.0  1.330429  2.420959  14.650238   57.000000     5.0\n","461    6930.0  1.328049  2.418518  14.646271   63.333333     5.0\n","462    6945.0  1.328033  2.411621  14.656220   65.666667     5.0\n","...       ...       ...       ...        ...         ...     ...\n","1480  24947.0  6.986115  1.834229  13.185440   65.000000     5.0\n","1481  24962.0  6.983322  1.860351  13.181610   61.000000     5.0\n","1482  24977.0  6.977661  1.855469  13.182022   63.000000     5.0\n","1483  24992.0  6.976059  1.863800  13.195205   67.500000     5.0\n","1484  25007.0  6.957992  1.848099  13.199081   67.000000     5.0\n","\n","[509 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"distributed-handling"},"source":[""],"id":"distributed-handling","execution_count":null,"outputs":[]}]}