{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1pH40itHTdra",
   "metadata": {
    "id": "1pH40itHTdra"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88Ve2wbXTfNL",
   "metadata": {
    "id": "88Ve2wbXTfNL"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PS7-hk5PTbUU",
   "metadata": {
    "id": "PS7-hk5PTbUU",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install wget\n",
    "\n",
    "!wget -r -nv -N -c -np https://physionet.org/files/sleep-accel/1.0.0/\n",
    "\n",
    "!mkdir ./download\n",
    "\n",
    "!mv ./physionet.org/files/sleep-accel/1.0.0/* download\n",
    "\n",
    "!rm -r ./physionet.org/\n",
    "\n",
    "!find ./download -name \"*.html\" -type f -delete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-wages",
   "metadata": {
    "id": "stylish-wages"
   },
   "source": [
    "Steps:\n",
    "\n",
    "- Download and unzip the dataset\n",
    "- Load the files\n",
    "- Pre-process the loaded files (crop to keep the part of interest)\n",
    "- Merge files from each user selecting a specific window time frame.\n",
    "- Export the resulting file to `.csv`.\n",
    "- Repeat the process for all the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "forbidden-forty",
   "metadata": {
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1622812730244,
     "user": {
      "displayName": "Carlos Gil",
      "photoUrl": "",
      "userId": "12309568779801871703"
     },
     "user_tz": -120
    },
    "id": "forbidden-forty"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "qmXPkoZWyyEm",
   "metadata": {
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1622814720128,
     "user": {
      "displayName": "Carlos Gil",
      "photoUrl": "",
      "userId": "12309568779801871703"
     },
     "user_tz": -120
    },
    "id": "qmXPkoZWyyEm"
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Error(Enum):\n",
    "    dir_not_empty = \"[Error]: the directory is not empty \"\n",
    "    match_number_users = \"[Error]: number of users in list does not match \"\n",
    "    match_user_id = \"[Error]: user id does not match between lists \"\n",
    "    match_length_arrays = \"[Error]: the length of the lists does not match \"\n",
    "    match_index = \"[Error]: indexes are mismatched \"\n",
    "    generic_error = \"[Error] \"\n",
    "\n",
    "    @staticmethod\n",
    "    def raise_error(type_error, value):\n",
    "        return type_error.value + str(value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cGfE03A-Uvvb",
   "metadata": {
    "id": "cGfE03A-Uvvb"
   },
   "source": [
    "###### Preparing paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-flood",
   "metadata": {
    "id": "expanded-flood"
   },
   "outputs": [],
   "source": [
    "# Dataset path. The directory where all the output datasets will be saved\n",
    "if not os.path.isdir('dataset'):\n",
    "    os.mkdir('dataset')\n",
    "else:\n",
    "    if len(os.listdir('dataset/')) > 0:\n",
    "        raise Exception(Error.raise_error(Error.dir_not_empty,  \"\\\"dataset\\\"\"))\n",
    "\n",
    "output_path = os.path.join(os.getcwd(), 'dataset')\n",
    "\n",
    "# Download path. The directory where the downloaded files are located after download\n",
    "# data_path = \"C:\\dev\\DATA\\MRH\"\n",
    "data_path = os.path.join(os.getcwd(), \"download\")\n",
    "\n",
    "motion_path = os.path.join(data_path, \"motion\")\n",
    "heart_rate_path = os.path.join(data_path, \"heart_rate\")\n",
    "labels_path = os.path.join(data_path, \"labels\")\n",
    "\n",
    "# Obtaining lists with all users in ascending ordered\n",
    "motion_list = sorted(os.listdir(motion_path))\n",
    "heart_rate_list = sorted(os.listdir(heart_rate_path))\n",
    "labels_list = sorted(os.listdir(labels_path))\n",
    "\n",
    "# Checking that we have data of the 31 users in all the lists created\n",
    "assert len(motion_list) == 31, Error.match_number_users.value\n",
    "assert len(heart_rate_list) == 31, Error.match_number_users.value\n",
    "assert len(labels_list) == 31, Error.match_number_users.value\n",
    "\n",
    "user_ids = []\n",
    "\n",
    "# Checking that the user ids match in order accross the three lists\n",
    "for item in range(len(motion_list)):\n",
    "    user_motion_id = re.search(\"\\d*\", motion_list[item])\n",
    "    user_heart_rate_id = re.search(\"\\d*\", heart_rate_list[item])\n",
    "    user_labels_id = re.search(\"\\d*\", labels_list[item])\n",
    "\n",
    "    assert user_motion_id.group(0) == user_heart_rate_id.group(0), Error.match_user_id.value\n",
    "    assert user_motion_id.group(0) == user_labels_id.group(0), Error.match_user_id.value\n",
    "\n",
    "    user_ids.append(user_motion_id.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GM9GSzduN12Z",
   "metadata": {
    "id": "GM9GSzduN12Z"
   },
   "outputs": [],
   "source": [
    "def generate_all_datasets(motion_interval=1, epoch_interval=15, verbose=False):\n",
    "\n",
    "time_acc = []\n",
    "\n",
    "for user in range(0, len(motion_list)):\n",
    "    id_match = re.search(\"\\d*\", motion_list[user])\n",
    "    user_id = id_match.group(0)\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    user_df = generate_dataset(os.path.join(motion_path, motion_list[user]), \n",
    "                 os.path.join(heart_rate_path, heart_rate_list[user]), \n",
    "                 os.path.join(labels_path, labels_list[user]), \n",
    "                 motion_interval, epoch_interval)\n",
    "    \n",
    "    # Saving the generated dataset\n",
    "    fname = os.path.join(output_path, 'dataset_' + user_ids[user] + \".csv\")\n",
    "\n",
    "    user_df.to_csv(fname, index = False, header=True)\n",
    "\n",
    "    time_acc.append(time.time() - start)\n",
    "\n",
    "    if verbose:\n",
    "        print('Dataset user id <{}> generated succesfully. Time: {:.0f} s'.format(user_id, time_acc[user]))\n",
    "\n",
    "\n",
    "print(f'\\nProcess complete! Total time execution: {datetime.timedelta(seconds=sum(time_acc))}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-contractor",
   "metadata": {
    "id": "earlier-contractor"
   },
   "outputs": [],
   "source": [
    "def generate_dataset(motion_user, heart_rate_user, labels_user, interval_peak=1, interval_epoch=30):\n",
    "\n",
    "    '''\n",
    "    It accepts three filenames from one user to generate the dataset. Interval stands for the time in seconds of windowing.\n",
    "    '''\n",
    "    \n",
    "    # --- Loading the txt files\n",
    "    motion = np.loadtxt(motion_user)\n",
    "    heart_rate = np.loadtxt(heart_rate_user, delimiter=',')\n",
    "    labels = np.loadtxt(labels_user)\n",
    "\n",
    "    \n",
    "    # --- Cropping to match the labelled list\n",
    "    motion = crop_to_offset(motion, labels)    \n",
    "    heart_rate = crop_to_offset(heart_rate, labels)\n",
    "    \n",
    "    # --- Pre-processing and merging\n",
    "    motion = get_summary_count(motion, interval_peak, interval_epoch)\n",
    "    \n",
    "    merged_arrays = merge_arrays(motion, heart_rate, labels, interval_epoch)\n",
    "    \n",
    "    \n",
    "    data_frame = pd.DataFrame(merged_arrays, columns=['Time', 'X', 'Y', 'Z', 'Heart Rate', 'Labels'])\n",
    "    \n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-zimbabwe",
   "metadata": {
    "id": "touched-zimbabwe"
   },
   "source": [
    "The raw data recorded with the Apple Watch (motion and heart rate) contains continiuous and uninterrumped measurements of one or more days, including the last night.\n",
    "\n",
    "Since the data corresponding to the last night underwent a proper labelling from the PSG results, it is necessary to crop the raw data only to that night (i.e. the list with labels). Anything else, will not be part of the generated dataset and will therefore be disregarded.\n",
    "\n",
    "This is handled by the function `crop_to_offset()`. This function carries out two tasks:\n",
    "\n",
    "1. It finds the last night measured within the array passed.\n",
    "2. For the last night, it finds the boundaries corresponding to the start and end of the labelled list.\n",
    "\n",
    "Then, the function returns the indexes where the array needs to be sliced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-cooperative",
   "metadata": {
    "id": "diagnostic-cooperative"
   },
   "outputs": [],
   "source": [
    "def crop_to_offset(array_to_crop, array_ref):\n",
    "    '''\n",
    "    This function takes two arrays, the first is the one to be cropped and the second one the reference to where to crop.\n",
    "    It returs a new array starting and ending where the indexes matched with the reference array.\n",
    "    '''\n",
    "\n",
    "    start_index, end_index = 0, 0\n",
    "    array_size = np.size(array_to_crop, 0)\n",
    "    cropped_array = []\n",
    "    \n",
    "    # --- Find the boundaries corresponding to the labelled list\n",
    "    first_item = array_ref[0][0]\n",
    "    last_item = array_ref[-1][0]\n",
    "    \n",
    "    last_item_found = False\n",
    "    \n",
    "    for item in range(array_size - 1, -1, -1):        \n",
    "        # find end index\n",
    "        if not last_item_found:\n",
    "            if array_to_crop[item][0] < last_item:\n",
    "                end_index = item + 1\n",
    "                last_item_found = True\n",
    "        \n",
    "        # find start index\n",
    "        if array_to_crop[item][0] < first_item:\n",
    "            start_index = item\n",
    "            break  # No more iteration is needed after finding end_index and start_index.\n",
    "    \n",
    "    cropped_array = array_to_crop[start_index:end_index]\n",
    "        \n",
    "    return cropped_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hUP-MlGNFCWg",
   "metadata": {
    "id": "hUP-MlGNFCWg"
   },
   "source": [
    "In order to compress the vast amount of data gathered from the IMU sensor, some pre-processing is required. Tipically, these types of sensors record at a high frequencies resulting in hundreds of measurements every single second. Research suggest that changes among sleep cycles tend to occur gradually within a few minutes. This also applies to the shift between NREM and REM, which is the variable of interest that is the focus of this application.\n",
    "\n",
    "Therefore, the time resolution in the raw dataset is too accurate and the function `get_summary_count()` will handle this. All this function does is first finds the peak values within a user-defined time interval (we are most interested in peak values from the accelerometer that contribute to more valuable information) and sum all the spikes that take place within a window or time (epoch), also defined by the user.\n",
    "\n",
    "An example might be that the function finds the spike that occur in every second and then sums all the spikes found within a window of 15 seconds. This process then is repeated until completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aSTcpB_WDxOV",
   "metadata": {
    "id": "aSTcpB_WDxOV"
   },
   "outputs": [],
   "source": [
    "def get_summary_count(array, peak_interval, sum_window):\n",
    "    '''\n",
    "    This function first finds the peak value for every peak_interval (s) of the passed array.\n",
    "    It then sums all the peak values within sum_window (s) and adds the sum to a new array.\n",
    "    It returns the resulting processed array back.\n",
    "    '''\n",
    "\n",
    "    array_size = np.size(array, 0)\n",
    "    peak_values_x, sum_peak_values_x = [], []\n",
    "    peak_values_y, sum_peak_values_y = [], []\n",
    "    peak_values_z, sum_peak_values_z = [], []\n",
    "    \n",
    "    # missing_indices = np.empty((0, 3), dtype=float)\n",
    "    max_value_x, max_value_y, max_value_z = 0, 0, 0\n",
    "    time_accumulate = []\n",
    "    accumulate = 0\n",
    "    last_interval = 0\n",
    "    count = 0\n",
    "\n",
    "    for item in range(array_size):\n",
    "        if (array[item][0] - last_interval) < peak_interval:\n",
    "            if abs(array[item][1]) > abs(max_value_x):  # New peak value found at x\n",
    "                max_value_x = abs(array[item][1])\n",
    "\n",
    "            if abs(array[item][2]) > abs(max_value_y):  # New peak value found at y\n",
    "                max_value_y = abs(array[item][2])\n",
    "\n",
    "            if abs(array[item][3]) > abs(max_value_z):  # New peak value found at z\n",
    "                max_value_z = abs(array[item][3])\n",
    "\n",
    "        # Gap found, do not continue with current window\n",
    "        elif (array[item][0] - last_interval) > (peak_interval*1.5):\n",
    "            # print(\"============================================\")\n",
    "            # print(f\"[{array[item-1][0]}, {array[item][0]}]: {(array[item][0] - last_interval)}\")\n",
    "            # print(\"============================================\")          \n",
    "            # print(\"acc: \", accumulate)\n",
    "            \n",
    "            accumulate = np.around(array[item][0], decimals=0)\n",
    "            # print(\"acc after: \", accumulate)\n",
    "            # print(\"count: \", count)\n",
    "\n",
    "            # reset for a new window time. Current unfinished window invalid.\n",
    "            peak_values_x = []\n",
    "            peak_values_y = []\n",
    "            peak_values_z = []            \n",
    "            max_value_x = 0\n",
    "            max_value_y = 0\n",
    "            max_value_z = 0\n",
    "            count = 0\n",
    "            last_interval = np.floor(array[item][0])\n",
    "            \n",
    "        else:\n",
    "            # end of peak interval\n",
    "            peak_values_x.append(max_value_x)\n",
    "            peak_values_y.append(max_value_y)\n",
    "            peak_values_z.append(max_value_z)\n",
    "\n",
    "            # reset interval values and increment count\n",
    "            last_interval = np.floor(array[item][0])\n",
    "            max_value_x = 0\n",
    "            max_value_y = 0\n",
    "            max_value_z = 0\n",
    "            count += 1\n",
    "\n",
    "        if count == sum_window:\n",
    "            sum_peak_values_x.append(np.sum(peak_values_x))\n",
    "            sum_peak_values_y.append(np.sum(peak_values_y))\n",
    "            sum_peak_values_z.append(np.sum(peak_values_z))\n",
    "            \n",
    "            accumulate = np.around(accumulate + sum_window, decimals=0)\n",
    "\n",
    "            # print(\"====== acc:\", accumulate, \"val:\", np.around(array[item][0]), \"diff:\", (accumulate - array[item][0]))\n",
    "            \n",
    "            # Ensure that the current accumulated time matches the arrays' time\n",
    "            assert abs(accumulate - np.around(array[item][0])) < 2, Error.raise_error(Error.match_index, abs(accumulate - np.around(array[item][0])))\n",
    "\n",
    "            # time_accumulate.append(time_accumulate[-1] + sum_window)\n",
    "            time_accumulate.append(accumulate)\n",
    "\n",
    "            # reset for a new window time\n",
    "            peak_values_x = []\n",
    "            peak_values_y = []\n",
    "            peak_values_z = []\n",
    "            count = 0\n",
    "    \n",
    "    assert len(time_accumulate) == len(sum_peak_values_x), Error.match_length_arrays.value\n",
    "    assert len(time_accumulate) == len(sum_peak_values_y), Error.match_length_arrays.value\n",
    "    assert len(time_accumulate) == len(sum_peak_values_z), Error.match_length_arrays.value\n",
    "    \n",
    "    return np.column_stack((time_accumulate, sum_peak_values_x, sum_peak_values_y, sum_peak_values_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-buyer",
   "metadata": {
    "id": "growing-buyer"
   },
   "source": [
    "Last step is merging the already pre-processed motion dataset with the heart rate and labels datasets corresponding to each user. \n",
    "\n",
    "For the heart rate dataset the following approach is taken. For each window from the motion time interval, gather all the heart rate values that were measured within that interval and get the mean value of all of them. If no heart rate value was measured within a window interval, the last known value will be taken. If there is a time gap bigger than the actual window's size, then discard that data up to the next available one.\n",
    "\n",
    "For the labels a simpler approach is taken. Since the time windows are meant to be always smaller or equal than the time interval of labels recorded, the label value will be unique. It can be either one that corresponds to that time window or the last one if missing in that interval. \n",
    "\n",
    "All the above-mentioned operations are carried out in the `merge_arrays()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-difference",
   "metadata": {
    "id": "invalid-difference"
   },
   "outputs": [],
   "source": [
    "def merge_arrays(motion, heart_rate, labels, interval_epoch):\n",
    "    # array_size = max(np.size(motion, 0), np.size(heart_rate, 0), np.size(labels, 0))\n",
    "    # Checking that the biggest arrray to iterate through at this point is heart_rate\n",
    "    # assert array_size == np.size(heart_rate, 0), Error.match_length_arrays\n",
    "    \n",
    "    # --- Heart Rate pre-processing\n",
    "    \n",
    "    heart_rate_size = np.size(heart_rate, 0)\n",
    "    motion_size = np.size(motion, 0)\n",
    "\n",
    "    new_heart_rate = []\n",
    "    heart_rate_acc = []\n",
    "    \n",
    "    inc = 0\n",
    "        \n",
    "    for i in range(heart_rate_size):        \n",
    "\n",
    "        if heart_rate[i][0] < motion[inc][0]:\n",
    "            heart_rate_acc.append(heart_rate[i][1])\n",
    "        else:\n",
    "            # If time interval is bigger than the window time, take the latest value of HR\n",
    "            if motion[inc][0] - motion[inc - 1][0] > interval_epoch:\n",
    "                heart_rate_acc = [heart_rate[i][1]]\n",
    "            \n",
    "            # Append the mean off all values recorded\n",
    "            new_heart_rate.append(sum(heart_rate_acc)/len(heart_rate_acc))\n",
    "            # print(f\"[{motion[inc][0]}, {sum(heart_rate_acc)/len(heart_rate_acc)}]: \")\n",
    "\n",
    "            heart_rate_acc = [heart_rate[i][1]] # include first item that gave the condition too\n",
    "            inc += 1    \n",
    "\n",
    "        # If the motion dattaset finishes but there is still heart rate dataset, dismiss the rest of the latter.\n",
    "        if inc == motion_size:\n",
    "            break\n",
    "    \n",
    "    # Removing exceding data of motion that is not available in heart rate.\n",
    "    if len(new_heart_rate) < np.size(motion, 0):\n",
    "        diff = abs(len(new_heart_rate) - np.size(motion, 0)) #3\n",
    "        last_row = np.size(motion, 0)\n",
    "        start = last_row - diff\n",
    "\n",
    "        subarray = np.arange(start, last_row)\n",
    "\n",
    "        motion = np.delete(motion, subarray, axis=0)\n",
    "        \n",
    "\n",
    "    \n",
    "    # --- Labels pre-processing\n",
    "    \n",
    "    # labels_size = np.size(labels, 0)\n",
    "    motion_size = np.size(motion, 0)  # this might be new if deletion was needed before\n",
    "    new_labels = []\n",
    "    \n",
    "    inc = 0\n",
    "\n",
    "    # print(labels_size)\n",
    "    \n",
    "    for j in range(motion_size):\n",
    "        if motion[j][0] > labels[inc][0]:\n",
    "            inc += 1\n",
    "        \n",
    "        new_labels.append(labels[inc][1])\n",
    "\n",
    "        # if inc == labels_size:\n",
    "        #    break\n",
    "    \n",
    "    \n",
    "    assert np.size(motion, 0) == len(new_heart_rate), Error.match_length_arrays.value\n",
    "    assert np.size(motion, 0) == len(new_labels), Error.match_length_arrays.value\n",
    "    \n",
    "    return np.column_stack((motion, new_heart_rate, new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qlto6gQSL5ab",
   "metadata": {
    "id": "Qlto6gQSL5ab"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "opposite-command",
   "metadata": {
    "id": "opposite-command"
   },
   "source": [
    "###### Saving sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-phone",
   "metadata": {
    "id": "requested-phone"
   },
   "outputs": [],
   "source": [
    "user_id = re.search(\"\\d*\", motion_list[0])\n",
    "\n",
    "fname = 'dataset_' + user_id.group(0) + \".csv\"\n",
    "\n",
    "user_1.to_csv(fname, index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-butler",
   "metadata": {
    "id": "academic-butler"
   },
   "source": [
    "###### Testing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-wilson",
   "metadata": {
    "id": "humanitarian-wilson",
    "outputId": "b087e0bb-3498-445f-e339-c0e112c2982c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>Heart Rate</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>6885.0</td>\n",
       "      <td>1.330994</td>\n",
       "      <td>2.418594</td>\n",
       "      <td>14.649231</td>\n",
       "      <td>58.333333</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>6900.0</td>\n",
       "      <td>1.325516</td>\n",
       "      <td>2.420059</td>\n",
       "      <td>14.650604</td>\n",
       "      <td>56.333333</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>6915.0</td>\n",
       "      <td>1.330429</td>\n",
       "      <td>2.420959</td>\n",
       "      <td>14.650238</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>6930.0</td>\n",
       "      <td>1.328049</td>\n",
       "      <td>2.418518</td>\n",
       "      <td>14.646271</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>6945.0</td>\n",
       "      <td>1.328033</td>\n",
       "      <td>2.411621</td>\n",
       "      <td>14.656220</td>\n",
       "      <td>65.666667</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>24947.0</td>\n",
       "      <td>6.986115</td>\n",
       "      <td>1.834229</td>\n",
       "      <td>13.185440</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>24962.0</td>\n",
       "      <td>6.983322</td>\n",
       "      <td>1.860351</td>\n",
       "      <td>13.181610</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>24977.0</td>\n",
       "      <td>6.977661</td>\n",
       "      <td>1.855469</td>\n",
       "      <td>13.182022</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>24992.0</td>\n",
       "      <td>6.976059</td>\n",
       "      <td>1.863800</td>\n",
       "      <td>13.195205</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>25007.0</td>\n",
       "      <td>6.957992</td>\n",
       "      <td>1.848099</td>\n",
       "      <td>13.199081</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Time         X         Y          Z  Heart Rate  Labels\n",
       "458    6885.0  1.330994  2.418594  14.649231   58.333333     5.0\n",
       "459    6900.0  1.325516  2.420059  14.650604   56.333333     5.0\n",
       "460    6915.0  1.330429  2.420959  14.650238   57.000000     5.0\n",
       "461    6930.0  1.328049  2.418518  14.646271   63.333333     5.0\n",
       "462    6945.0  1.328033  2.411621  14.656220   65.666667     5.0\n",
       "...       ...       ...       ...        ...         ...     ...\n",
       "1480  24947.0  6.986115  1.834229  13.185440   65.000000     5.0\n",
       "1481  24962.0  6.983322  1.860351  13.181610   61.000000     5.0\n",
       "1482  24977.0  6.977661  1.855469  13.182022   63.000000     5.0\n",
       "1483  24992.0  6.976059  1.863800  13.195205   67.500000     5.0\n",
       "1484  25007.0  6.957992  1.848099  13.199081   67.000000     5.0\n",
       "\n",
       "[509 rows x 6 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_1[user_1[\"Labels\"] > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-handling",
   "metadata": {
    "id": "distributed-handling"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "generate_dataset.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
