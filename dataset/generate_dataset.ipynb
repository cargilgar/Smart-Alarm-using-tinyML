{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"name":"generate_dataset.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"code","metadata":{"id":"1pH40itHTdra","executionInfo":{"status":"ok","timestamp":1623671879326,"user_tz":-120,"elapsed":334,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"id":"1pH40itHTdra","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"88Ve2wbXTfNL","executionInfo":{"status":"ok","timestamp":1623671886341,"user_tz":-120,"elapsed":224,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"id":"88Ve2wbXTfNL","execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"stylish-wages"},"source":["## Steps\n","\n","- Download and unzip the dataset into its corresponding directory.\n","- Load the `.txt` files.\n","- Pre-process the loaded files in accordance with some pre-defined requirements.\n","- Merge processed files from each user into one dataframe.\n","- Export the resulting dataframe to `.csv`.\n","- Repeat the process for all the users."],"id":"stylish-wages"},{"cell_type":"markdown","metadata":{"id":"a52ac485"},"source":["## Dataset download\n","Downloading the dataset from `https://physionet.org` and storing it locally in a folder called `download/`\n","\n","**Note:** If the dataset has been downloaded, do not run the following cell.\n","\n","You can download the dataset manually by going to the [website](https://www.physionet.org/content/sleep-accel/1.0.0/)."],"id":"a52ac485"},{"cell_type":"code","metadata":{"id":"PS7-hk5PTbUU","scrolled":false},"source":["!sudo apt-get install wget\n","\n","!wget -r -nv -N -c -np https://physionet.org/files/sleep-accel/1.0.0/\n","\n","!mkdir ./download\n","\n","!mv ./physionet.org/files/sleep-accel/1.0.0/* download\n","\n","!rm -r ./physionet.org/\n","\n","!find ./download -name \"*.html\" -type f -delete\n","\n","!mkdir ./output"],"id":"PS7-hk5PTbUU","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"forbidden-forty","executionInfo":{"status":"ok","timestamp":1623672106350,"user_tz":-120,"elapsed":19,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["import pandas as pd\n","import numpy as np\n","import os\n","import re\n","import time\n","import datetime\n","from enum import Enum"],"id":"forbidden-forty","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"qmXPkoZWyyEm","executionInfo":{"status":"ok","timestamp":1623672106351,"user_tz":-120,"elapsed":14,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["class Error(Enum):\n","    dir_not_empty = \"[Error]: the directory is not empty \"\n","    match_number_users = \"[Error]: number of users in list does not match \"\n","    match_user_id = \"[Error]: user id does not match between lists \"\n","    match_length_arrays = \"[Error]: the length of the lists does not match \"\n","    match_index = \"[Error]: indexes are mismatched \"\n","    generic_error = \"[Error] \"\n","\n","    @staticmethod\n","    def raise_error(type_error, value):\n","        return type_error.value + \": \" + str(value)\n"],"id":"qmXPkoZWyyEm","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cGfE03A-Uvvb"},"source":["## Layout and paths preparation\n","\n","The directory tree should resemble the following layout structure:\n","\n","```bash\n","dataset/\n","├── download/\n","│   ├── heart_rate/\n","│   ├── labels/\n","│   └── steps/\n","└── output/\n","    ├── dataset_user_1.csv\n","    ├── dataset_user_2.csv\n","    ⁞\n","    └── dataset_user_31.csv\n","```"],"id":"cGfE03A-Uvvb"},{"cell_type":"code","metadata":{"id":"expanded-flood","executionInfo":{"status":"ok","timestamp":1623672106770,"user_tz":-120,"elapsed":431,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["# Output path.\n","output_dir = 'output'\n","\n","output_path = os.path.join(os.getcwd(), output_dir)\n","\n","# Download path.\n","data_path = os.path.join(os.getcwd(), 'download')\n","\n","motion_path = os.path.join(data_path, 'motion')\n","heart_rate_path = os.path.join(data_path, 'heart_rate')\n","labels_path = os.path.join(data_path, 'labels')\n","\n","# Obtaining lists with all users in ascending ordered\n","motion_list = sorted(os.listdir(motion_path))\n","heart_rate_list = sorted(os.listdir(heart_rate_path))\n","labels_list = sorted(os.listdir(labels_path))\n","\n","# Checking that the downloaded data contains the 31 users in all the lists created\n","assert len(motion_list) == 31, Error.match_number_users.value\n","assert len(heart_rate_list) == 31, Error.match_number_users.value\n","assert len(labels_list) == 31, Error.match_number_users.value\n","\n","user_ids = []\n","\n","# Checking that the user ids match in order accross the three lists\n","for item in range(len(motion_list)):\n","    user_motion_id = re.search(\"\\d*\", motion_list[item])\n","    user_heart_rate_id = re.search(\"\\d*\", heart_rate_list[item])\n","    user_labels_id = re.search(\"\\d*\", labels_list[item])\n","\n","    assert user_motion_id.group(0) == user_heart_rate_id.group(0), Error.match_user_id.value\n","    assert user_motion_id.group(0) == user_labels_id.group(0), Error.match_user_id.value\n","\n","    user_ids.append(user_motion_id.group(0))"],"id":"expanded-flood","execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d31c16de"},"source":["## Creation of generate datasets function"],"id":"d31c16de"},{"cell_type":"code","metadata":{"id":"GM9GSzduN12Z","executionInfo":{"status":"ok","timestamp":1623672106770,"user_tz":-120,"elapsed":10,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["def generate_all_datasets(motion_interval=1, epoch_interval=15, labels_df=[], verbose=False):\n","    '''\n","    This function takes a total of four parameters: \n","        - motion_interval: the interval of time (s) from which retrieve a peak value\n","        - epoch_interval: the interval or window of time (s) that the generated dataset will have between instances.\n","        - labels_df: a list with the name of the columns of the generated datasets (6 labels). If not specified or less/more than 6 labels given, it takes default values.\n","        - verbose: if set to true, display some feedback of the process (the whole process might take several minutes).\n","    '''\n","    \n","    # Checking that the output directory is empty\n","    if len(os.listdir(output_dir)) > 0:\n","        print(f\"The folder {output_dir}/ is not empty\")\n","        input_val = input(\"Do you want to overwrite it? [y/n] \\n\")\n","\n","        if input_val == \"y\":\n","            file_list = [file for file in os.listdir(output_dir)]\n","            \n","            for file in file_list:\n","                os.remove(os.path.join(output_dir, file))\n","        else:\n","            raise Exception(Error.raise_error(Error.dir_not_empty,  output_dir))\n","\n","    if len(labels_df) != 6:\n","        labels_df = ['Time', 'X', 'Y', 'Z', 'Heart Rate', 'Labels']\n","\n","    time_acc = []\n","\n","    for user in range(0, len(user_ids)):\n","        \n","        start = time.time()\n","\n","        user_df = generate_dataset(os.path.join(motion_path, motion_list[user]), \n","                    os.path.join(heart_rate_path, heart_rate_list[user]), \n","                    os.path.join(labels_path, labels_list[user]), \n","                    motion_interval, epoch_interval, labels_df)\n","        \n","        # Saving the generated dataset\n","        fname = os.path.join(output_path, 'dataset_' + user_ids[user] + \".csv\")\n","\n","        user_df.to_csv(fname, index = False, header=True)\n","\n","        time_acc.append(time.time() - start)\n","\n","        if verbose:\n","            print('Dataset user id <{}> generated succesfully. Time: {:.0f} s'.format(user_ids[user], time_acc[user]))\n","\n","\n","    print(f'\\nProcess complete! Total time execution: {datetime.timedelta(seconds=sum(time_acc))}\\n')"],"id":"GM9GSzduN12Z","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"earlier-contractor","executionInfo":{"status":"ok","timestamp":1623672106772,"user_tz":-120,"elapsed":11,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["def generate_dataset(motion_user, heart_rate_user, labels_user, interval_peak, epoch_interval, labels_columns):\n","\n","    '''\n","    this function accepts three filenames from one user to generate the dataset as well as the pre-defined intervals.\n","    '''\n","    \n","    # --- Loading the txt files\n","    motion = np.loadtxt(motion_user)\n","    heart_rate = np.loadtxt(heart_rate_user, delimiter=',')\n","    labels = np.loadtxt(labels_user)\n","\n","    \n","    # --- Cropping to match the labelled list\n","    motion = crop_to_offset(motion, labels)    \n","    heart_rate = crop_to_offset(heart_rate, labels)\n","    \n","    # --- Pre-processing and merging\n","    motion = get_summary_count(motion, interval_peak, epoch_interval)\n","    \n","    merged_arrays = merge_arrays(motion, heart_rate, labels, epoch_interval)\n","    \n","    \n","    data_frame = pd.DataFrame(merged_arrays, columns=labels_columns)\n","    \n","    return data_frame"],"id":"earlier-contractor","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"touched-zimbabwe"},"source":["The raw data recorded with the Apple Watch (motion and heart rate) contains continiuous and uninterrumped measurements of one or more days, including the last night.\n","\n","Since the data corresponding to the last night underwent a proper labelling from the PSG results, it is necessary to crop the raw data only to that night (i.e. the list with labels). Anything else, will not be part of the generated dataset and will therefore be disregarded.\n","\n","This is handled by the function `crop_to_offset()`. This function carries out two tasks:\n","\n","1. It finds the last night measured within the array passed.\n","2. For the last night, it finds the boundaries corresponding to the start and end of the labelled list.\n","\n","Then, the function returns the indexes where the array needs to be sliced."],"id":"touched-zimbabwe"},{"cell_type":"code","metadata":{"id":"diagnostic-cooperative","executionInfo":{"status":"ok","timestamp":1623672106772,"user_tz":-120,"elapsed":9,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["def crop_to_offset(array_to_crop, array_ref):\n","    '''\n","    This function takes two arrays, the first is the one to be cropped and the second one the reference to where to crop.\n","    It returs a new array starting and ending where the indexes matched with the reference array.\n","    '''\n","\n","    start_index, end_index = 0, 0\n","    array_size = np.size(array_to_crop, 0)\n","    cropped_array = []\n","    \n","    # --- Find the boundaries corresponding to the labelled list\n","    first_item = array_ref[0][0]\n","    last_item = array_ref[-1][0]\n","    \n","    last_item_found = False\n","    \n","    for item in range(array_size - 1, -1, -1):        \n","        # find end index\n","        if not last_item_found:\n","            if array_to_crop[item][0] < last_item:\n","                end_index = item + 1\n","                last_item_found = True\n","        \n","        # find start index\n","        if array_to_crop[item][0] < first_item:\n","            start_index = item\n","            break  # No more iteration is needed after finding end_index and start_index.\n","    \n","    cropped_array = array_to_crop[start_index:end_index]\n","        \n","    return cropped_array"],"id":"diagnostic-cooperative","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hUP-MlGNFCWg"},"source":["In order to compress the vast amount of data gathered from the IMU sensor, some pre-processing is required. Tipically, these types of sensors record at a high frequencies resulting in hundreds of measurements every single second. Research suggest that changes among sleep cycles tend to occur gradually within a few minutes. This also applies to the shift between NREM and REM, which is the variable of interest that is the focus of this application.\n","\n","Therefore, the time resolution in the raw dataset is too accurate and the function `get_summary_count()` will handle this. All this function does is first finds the peak values within a user-defined time interval (we are most interested in peak values from the accelerometer that contribute to more valuable information) and sum all the spikes that take place within a window or time (epoch), also defined by the user.\n","\n","An example might be that the function finds the spike that occur in every second and then sums all the spikes found within a window of 15 seconds. This process then is repeated until completion."],"id":"hUP-MlGNFCWg"},{"cell_type":"code","metadata":{"id":"aSTcpB_WDxOV","executionInfo":{"status":"ok","timestamp":1623672106773,"user_tz":-120,"elapsed":9,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["def get_summary_count(array, peak_interval, sum_window):\n","    '''\n","    This function first finds the peak value for every peak_interval (s) of the passed array.\n","    It then sums all the peak values within sum_window (s) and adds the sum to a new array.\n","    It returns the resulting processed array back.\n","    '''\n","    \n","    THRESHOLD = 2\n","    array_size = np.size(array, 0)\n","    peak_values_x, sum_peak_values_x = [], []\n","    peak_values_y, sum_peak_values_y = [], []\n","    peak_values_z, sum_peak_values_z = [], []\n","    \n","    # missing_indices = np.empty((0, 3), dtype=float)\n","    max_value_x, max_value_y, max_value_z = 0, 0, 0\n","    time_accumulate = []\n","    accumulate = 0\n","    last_interval = 0\n","    count = 0\n","\n","    for item in range(array_size):\n","        if (array[item][0] - last_interval) < peak_interval:\n","            if abs(array[item][1]) > abs(max_value_x):  # New peak value found at x\n","                max_value_x = abs(array[item][1])\n","\n","            if abs(array[item][2]) > abs(max_value_y):  # New peak value found at y\n","                max_value_y = abs(array[item][2])\n","\n","            if abs(array[item][3]) > abs(max_value_z):  # New peak value found at z\n","                max_value_z = abs(array[item][3])\n","\n","        # Gap found, do not continue with current window\n","        elif (array[item][0] - last_interval) > (peak_interval*1.5):\n","                       \n","            accumulate = np.around(array[item][0], decimals=0)\n","            \n","            # reset for a new window time. Current unfinished window no longer valid.\n","            peak_values_x = []\n","            peak_values_y = []\n","            peak_values_z = []            \n","            max_value_x = 0\n","            max_value_y = 0\n","            max_value_z = 0\n","            count = 0\n","            last_interval = np.floor(array[item][0])\n","            \n","        else:\n","            # end of peak interval\n","            peak_values_x.append(max_value_x)\n","            peak_values_y.append(max_value_y)\n","            peak_values_z.append(max_value_z)\n","\n","            # reset interval values and increment count\n","            last_interval = np.floor(array[item][0])\n","            max_value_x = 0\n","            max_value_y = 0\n","            max_value_z = 0\n","            count += 1\n","\n","        if count == sum_window:\n","            sum_peak_values_x.append(np.sum(peak_values_x))\n","            sum_peak_values_y.append(np.sum(peak_values_y))\n","            sum_peak_values_z.append(np.sum(peak_values_z))\n","            \n","            accumulate = np.around(accumulate + sum_window, decimals=0)\n","\n","            # print(\"====== acc:\", accumulate, \"val:\", np.around(array[item][0]), \"diff:\", (accumulate - array[item][0]))\n","            \n","            # Ensure that the current accumulated time matches the arrays' time\n","            assert abs(accumulate - np.around(array[item][0])) < THRESHOLD, \\\n","            Error.raise_error(Error.match_index, abs(accumulate - np.around(array[item][0])))\n","\n","            time_accumulate.append(accumulate)\n","\n","            # reset for a new window time\n","            peak_values_x = []\n","            peak_values_y = []\n","            peak_values_z = []\n","            count = 0\n","    \n","    assert len(time_accumulate) == len(sum_peak_values_x), Error.match_length_arrays.value\n","    assert len(time_accumulate) == len(sum_peak_values_y), Error.match_length_arrays.value\n","    assert len(time_accumulate) == len(sum_peak_values_z), Error.match_length_arrays.value\n","    \n","    return np.column_stack((time_accumulate, sum_peak_values_x, sum_peak_values_y, sum_peak_values_z))"],"id":"aSTcpB_WDxOV","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"growing-buyer"},"source":["Last step is merging the already pre-processed motion dataset with the heart rate and labels datasets corresponding to each user. \n","\n","For the heart rate dataset the following approach is taken. For each window from the motion time interval, gather all the heart rate values that were measured within that interval and get the mean value of all of them. If no heart rate value was measured within a window interval, the last known value will be taken. If there is a time gap bigger than the actual window's size, then discard that data up to the next available one.\n","\n","For the labels a simpler approach is taken. Since the time windows are meant to be always smaller or equal than the time interval of labels recorded, the label value will be unique. It can be either one that corresponds to that time window or the last one if missing in that interval. \n","\n","All the above-mentioned operations are carried out in the `merge_arrays()` method."],"id":"growing-buyer"},{"cell_type":"code","metadata":{"id":"invalid-difference","executionInfo":{"status":"ok","timestamp":1623672106774,"user_tz":-120,"elapsed":10,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}}},"source":["def merge_arrays(motion, heart_rate, labels, interval_epoch):\n","        \n","    # --- Heart Rate pre-processing\n","    \n","    heart_rate_size = np.size(heart_rate, 0)\n","    motion_size = np.size(motion, 0)\n","\n","    new_heart_rate = []\n","    heart_rate_acc = []\n","    \n","    inc = 0\n","        \n","    for i in range(heart_rate_size):        \n","\n","        if heart_rate[i][0] < motion[inc][0]:\n","            heart_rate_acc.append(heart_rate[i][1])\n","        else:\n","            # If time interval is bigger than the window time, take the latest value of HR\n","            if motion[inc][0] - motion[inc - 1][0] > interval_epoch:\n","                heart_rate_acc = [heart_rate[i][1]]\n","            \n","            new_heart_rate.append(sum(heart_rate_acc)/len(heart_rate_acc))\n","\n","            heart_rate_acc = [heart_rate[i][1]] # include first item that gave the condition too\n","            inc += 1    \n","\n","        # If the motion dataset finishes but there is still heart rate dataset, dismiss the rest of the latter.\n","        if inc == motion_size:\n","            break\n","    \n","    # Removing exceeding data of motion that is not available in heart rate.\n","    if len(new_heart_rate) < np.size(motion, 0):\n","        diff = abs(len(new_heart_rate) - np.size(motion, 0)) #3\n","        last_row = np.size(motion, 0)\n","        start = last_row - diff\n","\n","        subarray = np.arange(start, last_row)\n","\n","        motion = np.delete(motion, subarray, axis=0)\n","        \n","\n","    # --- Labels pre-processing\n","    \n","    motion_size = np.size(motion, 0)  # motion_size might be new if deletion was needed before\n","    new_labels = []\n","    \n","    inc = 0\n","    \n","    for j in range(motion_size):\n","        if motion[j][0] > labels[inc][0]:\n","            inc += 1\n","        \n","        new_labels.append(labels[inc][1])\n","\n","        \n","    assert np.size(motion, 0) == len(new_heart_rate), Error.match_length_arrays.value\n","    assert np.size(motion, 0) == len(new_labels), Error.match_length_arrays.value\n","    \n","    return np.column_stack((motion, new_heart_rate, new_labels))"],"id":"invalid-difference","execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"54857484"},"source":["## Running the application"],"id":"54857484"},{"cell_type":"code","metadata":{"id":"b3c3633a"},"source":["generate_all_datasets(motion_interval=1, epoch_interval=60, verbose=True)"],"id":"b3c3633a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZdxG3HSuJaW5"},"source":["## Downloading the datasets"],"id":"ZdxG3HSuJaW5"},{"cell_type":"code","metadata":{"id":"BX9aIckkJXEn"},"source":["!zip -r /content/datsets.zip /content/output"],"id":"BX9aIckkJXEn","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"Kfaho2OtJX6q","executionInfo":{"status":"ok","timestamp":1623672679563,"user_tz":-120,"elapsed":28,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}},"outputId":"8ba97123-100c-4c94-b95e-55d67576904f"},"source":["from google.colab import files\n","files.download(\"/content/datsets.zip\")"],"id":"Kfaho2OtJX6q","execution_count":14,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_662e0d83-5545-4460-8ab1-8228747981db\", \"datsets.zip\", 321465)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]}]}