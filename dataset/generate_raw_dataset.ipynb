{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"name":"generate_raw_dataset.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"code","metadata":{"id":"1pH40itHTdra"},"source":["# Copyright 2021 Carlos Gil, Daniel Moreno.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"id":"1pH40itHTdra","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"88Ve2wbXTfNL"},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"id":"88Ve2wbXTfNL","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"stylish-wages"},"source":["## Steps\n","\n","- Download and unzip the dataset into its corresponding directory.\n","- Load the `.txt` files.\n","- Merge the files from each user into one dataframe.\n","- Export the resulting dataframe to `.csv`.\n","- Repeat the process for all the users."],"id":"stylish-wages"},{"cell_type":"markdown","metadata":{"id":"a52ac485"},"source":["## Dataset download\n","Downloading the dataset from `https://physionet.org` and storing it locally in a folder called `download/`\n","\n","**Note:** If the dataset has been downloaded, do not run the following cell.\n","\n","You can download the dataset manually by going to the [website](https://www.physionet.org/content/sleep-accel/1.0.0/)."],"id":"a52ac485"},{"cell_type":"code","metadata":{"id":"PS7-hk5PTbUU","scrolled":false},"source":["!sudo apt-get install wget\n","\n","!wget -r -nv -N -c -np https://physionet.org/files/sleep-accel/1.0.0/\n","\n","!mkdir ./download\n","\n","!mv ./physionet.org/files/sleep-accel/1.0.0/* download\n","\n","!rm -r ./physionet.org/\n","\n","!find ./download -name \"*.html\" -type f -delete\n","\n","!mkdir ./output"],"id":"PS7-hk5PTbUU","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"forbidden-forty"},"source":["import pandas as pd\n","import numpy as np\n","import os\n","import re\n","import time\n","import datetime\n","from enum import Enum"],"id":"forbidden-forty","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qmXPkoZWyyEm"},"source":["class Error(Enum):\n","    dir_not_empty = \"[Error]: the directory is not empty \"\n","    match_number_users = \"[Error]: number of users in list does not match \"\n","    match_user_id = \"[Error]: user id does not match between lists \"\n","    match_length_arrays = \"[Error]: the length of the lists does not match \"\n","    match_index = \"[Error]: indexes are mismatched \"\n","    generic_error = \"[Error] \"\n","\n","    @staticmethod\n","    def raise_error(type_error, value):\n","        return type_error.value + \": \" + str(value)\n"],"id":"qmXPkoZWyyEm","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cGfE03A-Uvvb"},"source":["## Layout and paths preparation\n","\n","The directory tree should resemble the following layout structure:\n","\n","```bash\n","dataset/\n","├── download/\n","│   ├── heart_rate/\n","│   ├── labels/\n","│   └── steps/\n","└── output/\n","    ├── dataset_user_1.csv\n","    ├── dataset_user_2.csv\n","    ⁞\n","    └── dataset_user_31.csv\n","```"],"id":"cGfE03A-Uvvb"},{"cell_type":"code","metadata":{"id":"expanded-flood"},"source":["# Output path.\n","output_dir = 'output'\n","\n","output_path = os.path.join(os.getcwd(), output_dir)\n","\n","# Download path.\n","data_path = os.path.join(os.getcwd(), 'download')\n","\n","motion_path = os.path.join(data_path, 'motion')\n","heart_rate_path = os.path.join(data_path, 'heart_rate')\n","labels_path = os.path.join(data_path, 'labels')\n","\n","# Obtaining lists with all users in ascending ordered\n","motion_list = sorted(os.listdir(motion_path))\n","heart_rate_list = sorted(os.listdir(heart_rate_path))\n","labels_list = sorted(os.listdir(labels_path))\n","\n","# Checking that the downloaded data contains the 31 users in all the lists created\n","assert len(motion_list) == 31, Error.match_number_users.value\n","assert len(heart_rate_list) == 31, Error.match_number_users.value\n","assert len(labels_list) == 31, Error.match_number_users.value\n","\n","user_ids = []\n","\n","# Checking that the user ids match in order accross the three lists\n","for item in range(len(motion_list)):\n","    user_motion_id = re.search(\"\\d*\", motion_list[item])\n","    user_heart_rate_id = re.search(\"\\d*\", heart_rate_list[item])\n","    user_labels_id = re.search(\"\\d*\", labels_list[item])\n","\n","    assert user_motion_id.group(0) == user_heart_rate_id.group(0), Error.match_user_id.value\n","    assert user_motion_id.group(0) == user_labels_id.group(0), Error.match_user_id.value\n","\n","    user_ids.append(user_motion_id.group(0))"],"id":"expanded-flood","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d31c16de"},"source":["## Creation of generate datasets function"],"id":"d31c16de"},{"cell_type":"code","metadata":{"id":"GM9GSzduN12Z"},"source":["def generate_all_datasets(labels_df=[], verbose=False):\n","    '''\n","    This function takes a total of four parameters: \n","        - labels_df: a list with the name of the columns of the generated datasets (6 labels). If not specified or less/more than 6 labels given, it takes default values.\n","        - verbose: if set to true, display some feedback of the process (the whole process might take several minutes).\n","    '''\n","    \n","    # Checking that the output directory is empty\n","    if len(os.listdir(output_dir)) > 0:\n","        print(f\"The folder {output_dir}/ is not empty\")\n","        input_val = input(\"Do you want to overwrite it? [y/n] \\n\")\n","\n","        if input_val == \"y\":\n","            file_list = [file for file in os.listdir(output_dir)]\n","            \n","            for file in file_list:\n","                os.remove(os.path.join(output_dir, file))\n","        else:\n","            raise Exception(Error.raise_error(Error.dir_not_empty,  output_dir))\n","\n","    if len(labels_df) != 6:\n","        labels_df = ['Time', 'X', 'Y', 'Z', 'Heart Rate', 'Labels']\n","\n","    time_acc = []\n","\n","    for user in range(0, len(user_ids)):\n","        \n","        start = time.time()\n","\n","        motion_user = os.path.join(motion_path, motion_list[user])\n","        heart_rate_user = os.path.join(heart_rate_path, heart_rate_list[user])\n","        labels_user = os.path.join(labels_path, labels_list[user])\n","\n","        user_df = generate_dataset(motion_user, heart_rate_user, labels_user, labels_df)\n","        \n","        # Saving the generated dataset\n","        fname = os.path.join(output_path, 'dataset_' + user_ids[user] + \".csv\")\n","\n","        user_df.to_csv(fname, index = False, header=True)\n","\n","        time_acc.append(time.time() - start)\n","\n","        if verbose:\n","            print('Dataset user id <{}> generated succesfully. Time: {:.0f} s'.format(user_ids[user], time_acc[user]))\n","\n","\n","    print(f'\\nProcess complete! Total time execution: {datetime.timedelta(seconds=sum(time_acc))}\\n')"],"id":"GM9GSzduN12Z","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BEJ7CGoNwbFY"},"source":["def generate_dataset(motion_user, heart_rate_user, labels_user, labels_columns):\n","\n","    '''\n","    this function accepts three filenames from one user to generate the dataset as well as the pre-defined intervals.\n","    '''\n","    \n","    # --- Loading the txt files\n","    motion = np.loadtxt(motion_user)\n","    heart_rate = np.loadtxt(heart_rate_user, delimiter=',')\n","    labels = np.loadtxt(labels_user)\n","\n","    # --- Cropping to match the labelled list\n","    motion = crop_to_offset(motion, labels)    \n","    heart_rate = crop_to_offset(heart_rate, labels)\n","\n","    # --- Merging\n","    merged_arrays = merge_arrays(motion, heart_rate, labels)\n","\n","    # --- Importing merged to a pandas dataframe\n","    data_frame = pd.DataFrame(merged_arrays, columns=labels_columns)\n","\n","    return data_frame"],"id":"BEJ7CGoNwbFY","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"touched-zimbabwe"},"source":["The raw data recorded with the Apple Watch (motion and heart rate) contains continiuous and uninterrumped measurements of one or more days, including the last night.\n","\n","Since the data corresponding to the last night underwent a proper labelling from the PSG results, it is necessary to crop the raw data only to that night (i.e. the list with labels). Anything else, will not be part of the generated dataset and will therefore be disregarded.\n","\n","This is handled by the function `crop_to_offset()`. This function carries out two tasks:\n","\n","1. It finds the last night measured within the array passed.\n","2. For the last night, it finds the boundaries corresponding to the start and end of the labelled list.\n","\n","Then, the function returns the indexes where the array needs to be sliced."],"id":"touched-zimbabwe"},{"cell_type":"code","metadata":{"id":"diagnostic-cooperative"},"source":["def crop_to_offset(array_to_crop, array_ref):\n","    '''\n","    This function takes two arrays, the first is the one to be cropped and the second one the reference to where to crop.\n","    It returs a new array starting and ending where the indexes matched with the reference array.\n","    '''\n","\n","    start_index, end_index = 0, 0\n","    array_size = np.size(array_to_crop, 0)\n","    cropped_array = []\n","    \n","    # --- Find the boundaries corresponding to the labelled list\n","    first_item = array_ref[0][0]\n","    last_item = array_ref[-1][0]\n","    \n","    last_item_found = False\n","    \n","    for item in range(array_size - 1, -1, -1):        \n","        # find end index\n","        if not last_item_found:\n","            if array_to_crop[item][0] < last_item:\n","                end_index = item + 1\n","                last_item_found = True\n","        \n","        # find start index\n","        if array_to_crop[item][0] < first_item:\n","            start_index = item\n","            break  # No more iteration is needed after finding end_index and start_index.\n","    \n","    cropped_array = array_to_crop[start_index:end_index]\n","        \n","    return cropped_array"],"id":"diagnostic-cooperative","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"growing-buyer"},"source":["Last step is merging the already pre-processed motion dataset with the heart rate and labels datasets corresponding to each user. \n","\n","For the heart rate dataset the following approach is taken. For each window from the motion time interval, gather all the heart rate values that were measured within that interval and get the mean value of all of them. If no heart rate value was measured within a window interval, the last known value will be taken. If there is a time gap bigger than the actual window's size, then discard that data up to the next available one.\n","\n","For the labels a simpler approach is taken. Since the time windows are meant to be always smaller or equal than the time interval of labels recorded, the label value will be unique. It can be either one that corresponds to that time window or the last one if missing in that interval. \n","\n","All the above-mentioned operations are carried out in the `merge_arrays()` method."],"id":"growing-buyer"},{"cell_type":"code","metadata":{"id":"invalid-difference"},"source":["def merge_arrays(motion, heart_rate, labels):\n","    \n","    motion_size = np.size(motion, 0)\n","    heart_rate_size = np.size(heart_rate, 0)\n","    labels_size = np.size(labels, 0)\n","\n","    new_heart_rate = []\n","    new_labels = []\n","    \n","    inc_heart_rate = 0\n","    inc_labels = 0\n","\n","    for i in range(motion_size):\n","\n","        # Appending heart rate \n","        if inc_heart_rate == heart_rate_size:\n","            pass\n","        elif motion[i][0] > heart_rate[inc_heart_rate][0]:\n","            inc_heart_rate += 1\n","        \n","        new_heart_rate.append(heart_rate[inc_heart_rate - 1][1])\n","\n","        # Appending labels\n","        if motion[i][0] > labels[inc_labels][0]:\n","            inc_labels += 1\n","\n","        new_labels.append(labels[inc_labels - 1][1])\n","\n","        \n","        # If the motion dataset finishes but there is still heart rate dataset, dismiss the rest of the latter.\n","        if inc_heart_rate >= motion_size:\n","            break\n","\n","        \n","    assert motion_size == len(new_heart_rate), Error.match_length_arrays.value\n","    assert motion_size == len(new_labels), Error.match_length_arrays.value\n","    \n","    return np.column_stack((motion, new_heart_rate, new_labels))"],"id":"invalid-difference","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"54857484"},"source":["## Running the application"],"id":"54857484"},{"cell_type":"code","metadata":{"id":"b3c3633a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627217805839,"user_tz":-120,"elapsed":872339,"user":{"displayName":"Carlos Gil","photoUrl":"","userId":"12309568779801871703"}},"outputId":"e21be27e-c18f-4755-d09c-47a50e0c9172"},"source":["generate_all_datasets(verbose=True)"],"id":"b3c3633a","execution_count":null,"outputs":[{"output_type":"stream","text":["Dataset user id <1066528> generated succesfully. Time: 24 s\n","Dataset user id <1360686> generated succesfully. Time: 28 s\n","Dataset user id <1449548> generated succesfully. Time: 33 s\n","Dataset user id <1455390> generated succesfully. Time: 30 s\n","Dataset user id <1818471> generated succesfully. Time: 30 s\n","Dataset user id <2598705> generated succesfully. Time: 29 s\n","Dataset user id <2638030> generated succesfully. Time: 38 s\n","Dataset user id <3509524> generated succesfully. Time: 19 s\n","Dataset user id <3997827> generated succesfully. Time: 34 s\n","Dataset user id <4018081> generated succesfully. Time: 20 s\n","Dataset user id <4314139> generated succesfully. Time: 29 s\n","Dataset user id <4426783> generated succesfully. Time: 33 s\n","Dataset user id <46343> generated succesfully. Time: 18 s\n","Dataset user id <5132496> generated succesfully. Time: 20 s\n","Dataset user id <5383425> generated succesfully. Time: 6 s\n","Dataset user id <5498603> generated succesfully. Time: 24 s\n","Dataset user id <5797046> generated succesfully. Time: 33 s\n","Dataset user id <6220552> generated succesfully. Time: 34 s\n","Dataset user id <759667> generated succesfully. Time: 20 s\n","Dataset user id <7749105> generated succesfully. Time: 6 s\n","Dataset user id <781756> generated succesfully. Time: 29 s\n","Dataset user id <8000685> generated succesfully. Time: 29 s\n","Dataset user id <8173033> generated succesfully. Time: 29 s\n","Dataset user id <8258170> generated succesfully. Time: 6 s\n","Dataset user id <844359> generated succesfully. Time: 28 s\n","Dataset user id <8530312> generated succesfully. Time: 31 s\n","Dataset user id <8686948> generated succesfully. Time: 29 s\n","Dataset user id <8692923> generated succesfully. Time: 33 s\n","Dataset user id <9106476> generated succesfully. Time: 29 s\n","Dataset user id <9618981> generated succesfully. Time: 95 s\n","Dataset user id <9961348> generated succesfully. Time: 28 s\n","\n","Process complete! Total time execution: 0:14:32.094211\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZdxG3HSuJaW5"},"source":["## Downloading the datasets"],"id":"ZdxG3HSuJaW5"},{"cell_type":"code","metadata":{"id":"BX9aIckkJXEn"},"source":["!zip -r /content/datasets.zip /content/output"],"id":"BX9aIckkJXEn","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kfaho2OtJX6q"},"source":["from google.colab import files\n","files.download(\"/content/datasets.zip\")"],"id":"Kfaho2OtJX6q","execution_count":null,"outputs":[]}]}